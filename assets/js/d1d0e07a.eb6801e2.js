"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[521],{7637:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"digital-twin/unity-for-high-fidelity-hri","title":"Unity for High-Fidelity HRI","description":"Learning Objectives","source":"@site/docs/02-digital-twin/03-unity-for-high-fidelity-hri.md","sourceDirName":"02-digital-twin","slug":"/digital-twin/unity-for-high-fidelity-hri","permalink":"/digital-twin/unity-for-high-fidelity-hri","draft":false,"unlisted":false,"editUrl":"https://github.com/mfsrajput/AI-And-Robotic-Hackathoon/edit/main/docs/02-digital-twin/03-unity-for-high-fidelity-hri.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Unity for High-Fidelity HRI"},"sidebar":"moduleSidebar","previous":{"title":"Sensor Simulation (LiDAR, Depth, IMU)","permalink":"/digital-twin/simulating-sensors-lidar-imu-depth"},"next":{"title":"Creating Complete Digital Twins","permalink":"/digital-twin/creating-complete-digital-twins"}}');var a=i(4848),r=i(8453);const s={sidebar_position:3,title:"Unity for High-Fidelity HRI"},o="Unity for High-Fidelity HRI",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Theory: Unity for Robotics Simulation",id:"theory-unity-for-robotics-simulation",level:2},{value:"Rendering Pipeline and Fidelity",id:"rendering-pipeline-and-fidelity",level:3},{value:"Physics Simulation in Unity",id:"physics-simulation-in-unity",level:3},{value:"Unity-ROS Integration",id:"unity-ros-integration",level:3},{value:"Practice: Creating High-Fidelity HRI Environments in Unity",id:"practice-creating-high-fidelity-hri-environments-in-unity",level:2},{value:"Setting Up Unity for Robotics",id:"setting-up-unity-for-robotics",level:3},{value:"Unity Scene Configuration for HRI",id:"unity-scene-configuration-for-hri",level:3},{value:"Creating Unity Animation Controllers for Humanoid Robots",id:"creating-unity-animation-controllers-for-humanoid-robots",level:3},{value:"Active Learning Exercise",id:"active-learning-exercise",level:2},{value:"Worked Example: Black-box to Glass-box - Implementing a Unity-ROS Bridge",id:"worked-example-black-box-to-glass-box---implementing-a-unity-ros-bridge",level:2},{value:"Black-box View",id:"black-box-view",level:3},{value:"Glass-box Implementation",id:"glass-box-implementation",level:3},{value:"Understanding the Implementation",id:"understanding-the-implementation",level:3},{value:"Tiered Assessments",id:"tiered-assessments",level:2},{value:"Tier 1: Basic Understanding",id:"tier-1-basic-understanding",level:3},{value:"Tier 2: Application",id:"tier-2-application",level:3},{value:"Tier 3: Analysis and Synthesis",id:"tier-3-analysis-and-synthesis",level:3},{value:"Mermaid Diagram",id:"mermaid-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(n){const e={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"unity-for-high-fidelity-hri",children:"Unity for High-Fidelity HRI"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Design and implement realistic 3D environments for humanoid robot simulation in Unity"}),"\n",(0,a.jsx)(e.li,{children:"Create high-fidelity visual rendering that matches real-world lighting and materials"}),"\n",(0,a.jsx)(e.li,{children:"Implement Unity-ROS communication bridges for real-time robot control"}),"\n",(0,a.jsx)(e.li,{children:"Develop intuitive human-robot interaction interfaces in Unity"}),"\n",(0,a.jsx)(e.li,{children:"Optimize Unity scenes for real-time performance with complex humanoid models"}),"\n",(0,a.jsx)(e.li,{children:"Integrate Unity with Gazebo for hybrid simulation approaches"}),"\n",(0,a.jsx)(e.li,{children:"Validate Unity simulation results against real-world HRI scenarios"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(e.p,{children:"Unity has emerged as a powerful platform for creating high-fidelity simulations with photorealistic rendering, making it ideal for human-robot interaction (HRI) research and development. While Gazebo excels at physics simulation, Unity provides superior visual fidelity, advanced rendering capabilities, and intuitive interface design tools that are essential for creating believable digital twins for HRI applications."}),"\n",(0,a.jsx)(e.p,{children:"For humanoid robotics, Unity offers the ability to create immersive environments where humans can interact with virtual robots in realistic settings. This high-fidelity visualization is crucial for applications such as robot training, user interface design, safety validation, and public demonstrations. Unity's real-time rendering engine, combined with its extensive asset library and development tools, enables the creation of compelling digital twins that accurately represent both the robot's appearance and its interactions with the environment."}),"\n",(0,a.jsx)(e.h2,{id:"theory-unity-for-robotics-simulation",children:"Theory: Unity for Robotics Simulation"}),"\n",(0,a.jsx)(e.h3,{id:"rendering-pipeline-and-fidelity",children:"Rendering Pipeline and Fidelity"}),"\n",(0,a.jsx)(e.p,{children:"Unity's rendering pipeline consists of several stages that contribute to visual fidelity:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Geometry Processing"}),": Handling 3D models, transformations, and culling"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Shading"}),": Calculating lighting, materials, and surface properties"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Lighting"}),": Global illumination, shadows, and reflections"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Post-Processing"}),": Effects like bloom, depth of field, and color correction"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"For HRI applications, photorealistic rendering helps bridge the reality gap between simulation and real-world interactions, making the simulation more valuable for training and validation."}),"\n",(0,a.jsx)(e.h3,{id:"physics-simulation-in-unity",children:"Physics Simulation in Unity"}),"\n",(0,a.jsx)(e.p,{children:"While Unity's physics engine (based on NVIDIA PhysX) is capable, it's typically used in conjunction with specialized robotics simulators like Gazebo for accurate rigid body dynamics. Unity excels at visualizing physics results rather than computing them, making it ideal for the presentation layer of a digital twin system."}),"\n",(0,a.jsx)(e.h3,{id:"unity-ros-integration",children:"Unity-ROS Integration"}),"\n",(0,a.jsx)(e.p,{children:"The Unity Robotics Hub provides tools for connecting Unity to ROS/ROS 2 systems:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"ROS-TCP-Connector"}),": Establishes communication between Unity and ROS"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"URDF Importer"}),": Imports robot models from URDF files"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robotics Messages"}),": Standard message types for Unity-ROS communication"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Simulation Framework"}),": Tools for creating simulation environments"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"practice-creating-high-fidelity-hri-environments-in-unity",children:"Practice: Creating High-Fidelity HRI Environments in Unity"}),"\n",(0,a.jsx)(e.h3,{id:"setting-up-unity-for-robotics",children:"Setting Up Unity for Robotics"}),"\n",(0,a.jsx)(e.p,{children:"To create Unity scenes for HRI, we'll need to set up the environment with appropriate lighting, materials, and robot models. Here's how to structure a basic Unity scene for humanoid robot simulation:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Creating a Unity Scene Structure"}),":"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"First, let's create a basic Unity scene configuration. Since we can't directly create Unity scene files here, I'll provide the C# scripts and configuration that would be used in Unity:"}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/Assets/Scripts/RobotController.cs"})," (simulated Unity script):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Geometry;\n\npublic class RobotController : MonoBehaviour\n{\n    // Robot joint transforms\n    public Transform headJoint;\n    public Transform leftShoulder;\n    public Transform leftElbow;\n    public Transform leftWrist;\n    public Transform rightShoulder;\n    public Transform rightElbow;\n    public Transform rightWrist;\n    public Transform leftHip;\n    public Transform leftKnee;\n    public Transform leftAnkle;\n    public Transform rightHip;\n    public Transform rightKnee;\n    public Transform rightAnkle;\n\n    // ROS connection\n    private ROSConnection ros;\n    private string robotNamespace = "/simple_humanoid";\n\n    // Joint positions\n    private float[] jointPositions = new float[12];\n    private string[] jointNames = {\n        "head_joint",\n        "left_shoulder_joint", "left_elbow_joint", "left_wrist_joint",\n        "right_shoulder_joint", "right_elbow_joint", "right_wrist_joint",\n        "left_hip_joint", "left_knee_joint", "left_ankle_joint",\n        "right_hip_joint", "right_knee_joint", "right_ankle_joint"\n    };\n\n    void Start()\n    {\n        // Get ROS connection\n        ros = ROSConnection.instance;\n\n        // Subscribe to joint states\n        ros.Subscribe<sensor_msgs.JointStateMsg>(robotNamespace + "/joint_states", JointStateCallback);\n    }\n\n    void JointStateCallback(sensor_msgs.JointStateMsg jointState)\n    {\n        // Update joint positions based on ROS messages\n        for (int i = 0; i < jointState.name.Count; i++)\n        {\n            string jointName = jointState.name[i];\n            float jointPosition = (float)jointState.position[i];\n\n            // Map joint names to transforms\n            switch (jointName)\n            {\n                case "head_joint":\n                    if (headJoint != null) headJoint.localRotation = Quaternion.Euler(0, jointPosition * Mathf.Rad2Deg, 0);\n                    break;\n                case "left_shoulder_joint":\n                    if (leftShoulder != null) leftShoulder.localRotation = Quaternion.Euler(0, 0, jointPosition * Mathf.Rad2Deg);\n                    break;\n                case "left_elbow_joint":\n                    if (leftElbow != null) leftElbow.localRotation = Quaternion.Euler(0, jointPosition * Mathf.Rad2Deg, 0);\n                    break;\n                // Add more joint mappings as needed\n            }\n        }\n    }\n\n    void Update()\n    {\n        // Any additional update logic\n    }\n}\n'})}),"\n",(0,a.jsxs)(e.ol,{start:"2",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Creating a Material System for Realistic Rendering"}),":"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/Assets/Scripts/MaterialManager.cs"})," (simulated Unity script):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\n\npublic class MaterialManager : MonoBehaviour\n{\n    [Header("Robot Materials")]\n    public Material headMaterial;\n    public Material torsoMaterial;\n    public Material limbMaterial;\n\n    [Header("Environment Materials")]\n    public Material floorMaterial;\n    public Material wallMaterial;\n\n    [Header("Real-time Settings")]\n    public bool useRealisticLighting = true;\n    public float materialSmoothness = 0.8f;\n\n    void Start()\n    {\n        SetupMaterials();\n    }\n\n    void SetupMaterials()\n    {\n        if (headMaterial != null)\n        {\n            headMaterial.SetFloat("_Smoothness", materialSmoothness);\n            headMaterial.EnableKeyword("_METALLICGLOSSMAP");\n        }\n\n        if (torsoMaterial != null)\n        {\n            torsoMaterial.SetFloat("_Smoothness", materialSmoothness * 0.7f);\n            torsoMaterial.EnableKeyword("_NORMALMAP");\n        }\n\n        if (limbMaterial != null)\n        {\n            limbMaterial.SetFloat("_Smoothness", materialSmoothness * 0.6f);\n        }\n\n        if (floorMaterial != null)\n        {\n            floorMaterial.EnableKeyword("_PARALLAXMAP");\n            floorMaterial.SetFloat("_BumpScale", 2.0f);\n        }\n    }\n\n    public void UpdateLightingConditions(float timeOfDay)\n    {\n        // Adjust lighting based on time of day for realistic rendering\n        float intensity = Mathf.Lerp(0.5f, 1.5f, Mathf.InverseLerp(6, 18, timeOfDay));\n        RenderSettings.ambientIntensity = intensity;\n    }\n}\n'})}),"\n",(0,a.jsxs)(e.ol,{start:"3",children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Creating an HRI Interface"}),":"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/Assets/Scripts/HRIInterface.cs"})," (simulated Unity script):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\nusing UnityEngine.UI;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\nusing RosMessageTypes.Geometry;\n\npublic class HRIInterface : MonoBehaviour\n{\n    [Header("UI Elements")]\n    public Text statusText;\n    public Text jointInfoText;\n    public Slider speedSlider;\n    public Button walkButton;\n    public Button stopButton;\n\n    [Header("Interaction Settings")]\n    public float interactionDistance = 3.0f;\n    public LayerMask interactionLayer;\n\n    private ROSConnection ros;\n    private string robotNamespace = "/simple_humanoid";\n\n    void Start()\n    {\n        ros = ROSConnection.instance;\n\n        // Set up UI event listeners\n        if (walkButton != null)\n            walkButton.onClick.AddListener(StartWalking);\n\n        if (stopButton != null)\n            stopButton.onClick.AddListener(StopRobot);\n\n        if (speedSlider != null)\n            speedSlider.onValueChanged.AddListener(OnSpeedChanged);\n\n        UpdateStatus("Robot connected and ready");\n    }\n\n    void Update()\n    {\n        // Check for user interaction with the robot\n        CheckUserInteraction();\n    }\n\n    void CheckUserInteraction()\n    {\n        RaycastHit hit;\n        Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition);\n\n        if (Physics.Raycast(ray, out hit, interactionDistance, interactionLayer))\n        {\n            if (Input.GetMouseButtonDown(0)) // Left click\n            {\n                // Send interaction command to robot\n                SendInteractionCommand(hit.transform.name);\n            }\n        }\n    }\n\n    void SendInteractionCommand(string target)\n    {\n        // Create and send interaction message\n        std_msgs.StringMsg interactionMsg = new std_msgs.StringMsg();\n        interactionMsg.data = "interaction_with_" + target;\n\n        ros.Send<Std_msgs.StringMsg>(robotNamespace + "/interaction", interactionMsg);\n        UpdateStatus("Interaction sent: " + target);\n    }\n\n    void StartWalking()\n    {\n        geometry_msgs.TwistMsg cmd = new geometry_msgs.TwistMsg();\n        cmd.linear.x = speedSlider.value;\n        cmd.angular.z = 0.0f;\n\n        ros.Send<geometry_msgs.TwistMsg>(robotNamespace + "/cmd_vel", cmd);\n        UpdateStatus("Walking started");\n    }\n\n    void StopRobot()\n    {\n        geometry_msgs.TwistMsg cmd = new geometry_msgs.TwistMsg();\n        cmd.linear.x = 0.0f;\n        cmd.angular.z = 0.0f;\n\n        ros.Send<geometry_msgs.TwistMsg>(robotNamespace + "/cmd_vel", cmd);\n        UpdateStatus("Robot stopped");\n    }\n\n    void OnSpeedChanged(float value)\n    {\n        UpdateStatus("Speed changed to: " + value.ToString("F2"));\n    }\n\n    void UpdateStatus(string message)\n    {\n        if (statusText != null)\n            statusText.text = message;\n    }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"unity-scene-configuration-for-hri",children:"Unity Scene Configuration for HRI"}),"\n",(0,a.jsx)(e.p,{children:"Create a configuration file that would set up the Unity scene for HRI:"}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/config/unity_hri_config.json"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-json",children:'{\n  "scene_settings": {\n    "render_pipeline": "urp",\n    "quality_level": "high",\n    "shadow_resolution": "high",\n    "reflection_probes": true,\n    "realtime_global_illumination": true\n  },\n  "robot_visualization": {\n    "lod_levels": [\n      {"distance": 0.0, "renderers": ["detailed_model"]},\n      {"distance": 10.0, "renderers": ["simplified_model"]},\n      {"distance": 30.0, "renderers": ["bounding_box"]}\n    ],\n    "material_properties": {\n      "metallic": 0.7,\n      "smoothness": 0.8,\n      "normal_map_strength": 1.0\n    }\n  },\n  "lighting_settings": {\n    "environment_type": "indoor",\n    "lighting_conditions": {\n      "daytime": {\n        "main_light_intensity": 1.2,\n        "ambient_light_color": [0.4, 0.4, 0.45, 1.0],\n        "reflection_intensity": 1.0\n      },\n      "nighttime": {\n        "main_light_intensity": 0.6,\n        "ambient_light_color": [0.15, 0.15, 0.2, 1.0],\n        "reflection_intensity": 0.5\n      }\n    }\n  },\n  "hri_interaction": {\n    "interaction_modes": ["gaze", "gesture", "voice", "touch"],\n    "interaction_distance": 3.0,\n    "feedback_types": ["visual", "audio", "haptic"],\n    "safety_zones": [\n      {\n        "name": "safe_zone",\n        "radius": 1.0,\n        "color": [0.2, 0.8, 0.2, 0.3]\n      },\n      {\n        "name": "warning_zone",\n        "radius": 2.0,\n        "color": [0.8, 0.6, 0.2, 0.3]\n      },\n      {\n        "name": "danger_zone",\n        "radius": 0.5,\n        "color": [0.8, 0.2, 0.2, 0.3]\n      }\n    ]\n  },\n  "performance_settings": {\n    "target_frame_rate": 60,\n    "lod_bias": 1.0,\n    "anisotropic_filtering": 2,\n    "anti_aliasing": "fxaa",\n    "shadow_distance": 50.0,\n    "view_distance": 100.0\n  },\n  "ros_integration": {\n    "tcp_endpoint": "127.0.0.1:10000",\n    "message_buffer_size": 1024,\n    "connection_timeout": 10.0,\n    "topics": {\n      "joint_states": "/simple_humanoid/joint_states",\n      "cmd_vel": "/simple_humanoid/cmd_vel",\n      "sensor_data": "/simple_humanoid/sensor_data",\n      "interaction": "/simple_humanoid/interaction"\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"creating-unity-animation-controllers-for-humanoid-robots",children:"Creating Unity Animation Controllers for Humanoid Robots"}),"\n",(0,a.jsx)(e.p,{children:"Create an animation configuration that would be used in Unity:"}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/config/robot_animation_config.json"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-json",children:'{\n  "animation_layers": [\n    {\n      "name": "Locomotion",\n      "weight": 1.0,\n      "states": [\n        {\n          "name": "Idle",\n          "motion_type": "idle",\n          "transition_duration": 0.2,\n          "parameters": ["speed", "direction"]\n        },\n        {\n          "name": "Walk",\n          "motion_type": "walk",\n          "transition_duration": 0.3,\n          "parameters": ["speed", "direction"]\n        },\n        {\n          "name": "Turn",\n          "motion_type": "turn",\n          "transition_duration": 0.2,\n          "parameters": ["turn_angle"]\n        },\n        {\n          "name": "Gesture",\n          "motion_type": "gesture",\n          "transition_duration": 0.1,\n          "parameters": ["gesture_type"]\n        }\n      ]\n    },\n    {\n      "name": "UpperBody",\n      "weight": 0.7,\n      "states": [\n        {\n          "name": "ArmMovement",\n          "motion_type": "arm_control",\n          "transition_duration": 0.1,\n          "parameters": ["left_arm_pos", "right_arm_pos"]\n        },\n        {\n          "name": "HeadTracking",\n          "motion_type": "head_control",\n          "transition_duration": 0.1,\n          "parameters": ["look_target"]\n        }\n      ]\n    }\n  ],\n  "animation_parameters": {\n    "speed": {"min": 0.0, "max": 2.0, "type": "float"},\n    "direction": {"min": -1.0, "max": 1.0, "type": "float"},\n    "turn_angle": {"min": -180.0, "max": 180.0, "type": "float"},\n    "gesture_type": {"type": "int", "values": {"wave": 0, "point": 1, "nod": 2, "shake": 3}},\n    "left_arm_pos": {"min": -3.0, "max": 3.0, "type": "float"},\n    "right_arm_pos": {"min": -3.0, "max": 3.0, "type": "float"},\n    "look_target": {"type": "vector3"}\n  },\n  "blend_trees": {\n    "walk_blend": {\n      "parameter": "speed",\n      "min_speed": 0.0,\n      "max_speed": 1.0,\n      "sub_states": [\n        {"threshold": 0.0, "state": "idle"},\n        {"threshold": 0.1, "state": "walk_start"},\n        {"threshold": 0.5, "state": "walk_normal"},\n        {"threshold": 1.0, "state": "walk_fast"}\n      ]\n    }\n  }\n}\n'})}),"\n",(0,a.jsx)(e.h2,{id:"active-learning-exercise",children:"Active Learning Exercise"}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Exercise: HRI Interface Design"})}),"\n",(0,a.jsx)(e.p,{children:"Design and implement an HRI interface in Unity that allows a human user to control a humanoid robot through:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Voice commands (simulate with keyboard input)"}),"\n",(0,a.jsx)(e.li,{children:"Gesture recognition (simulate with mouse/keyboard input)"}),"\n",(0,a.jsx)(e.li,{children:"Visual feedback systems"}),"\n",(0,a.jsx)(e.li,{children:"Safety monitoring features"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Consider how different users (children, elderly, people with disabilities) might interact with the robot and design inclusive interfaces. Create a Unity scene that demonstrates your design and test it with different interaction scenarios."}),"\n",(0,a.jsx)(e.h2,{id:"worked-example-black-box-to-glass-box---implementing-a-unity-ros-bridge",children:"Worked Example: Black-box to Glass-box - Implementing a Unity-ROS Bridge"}),"\n",(0,a.jsx)(e.h3,{id:"black-box-view",children:"Black-box View"}),"\n",(0,a.jsx)(e.p,{children:"We'll create a Unity-ROS bridge that allows real-time control of a humanoid robot model in Unity using ROS commands. The black-box view is: ROS sends joint state messages, and the Unity robot model updates in real-time to reflect those positions."}),"\n",(0,a.jsx)(e.h3,{id:"glass-box-implementation",children:"Glass-box Implementation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.strong,{children:"Create a ROS bridge manager:"})}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/src/ros_bridge_manager.cpp"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/joint_state.hpp>\n#include <geometry_msgs/msg/twist.hpp>\n#include <std_msgs/msg/string.hpp>\n#include <iostream>\n#include <string>\n#include <vector>\n#include <map>\n#include <thread>\n#include <mutex>\n\nclass ROSBridgeManager : public rclcpp::Node\n{\npublic:\n    ROSBridgeManager() : Node("unity_ros_bridge")\n    {\n        // Declare parameters\n        this->declare_parameter<std::string>("robot_namespace", "/simple_humanoid");\n        this->declare_parameter<std::string>("unity_endpoint", "127.0.0.1:10000");\n        this->declare_parameter<double>("publish_rate", 30.0);\n\n        // Get parameters\n        robot_namespace_ = this->get_parameter("robot_namespace").as_string();\n        unity_endpoint_ = this->get_parameter("unity_endpoint").as_string();\n        publish_rate_ = this->get_parameter("publish_rate").as_double();\n\n        // Create publishers and subscribers\n        joint_state_sub_ = this->create_subscription<sensor_msgs::msg::JointState>(\n            robot_namespace_ + "/joint_states",\n            10,\n            std::bind(&ROSBridgeManager::jointStateCallback, this, std::placeholders::_1));\n\n        cmd_vel_sub_ = this->create_subscription<geometry_msgs::msg::Twist>(\n            robot_namespace_ + "/cmd_vel",\n            10,\n            std::bind(&ROSBridgeManager::cmdVelCallback, this, std::placeholders::_1));\n\n        unity_data_pub_ = this->create_publisher<std_msgs::msg::String>(\n            "/unity_robot_data", 10);\n\n        RCLCPP_INFO(this->get_logger(), "Unity-ROS Bridge initialized");\n\n        // Start the Unity communication thread\n        unity_thread_ = std::thread(&ROSBridgeManager::unityCommunicationLoop, this);\n    }\n\n    ~ROSBridgeManager()\n    {\n        if (unity_thread_.joinable()) {\n            unity_thread_.join();\n        }\n    }\n\nprivate:\n    void jointStateCallback(const sensor_msgs::msg::JointState::SharedPtr msg)\n    {\n        std::lock_guard<std::mutex> lock(joint_state_mutex_);\n\n        // Store joint names and positions\n        joint_names_ = msg->name;\n        joint_positions_ = msg->position;\n\n        // Format data for Unity\n        std::string unity_data = formatJointDataForUnity();\n\n        // Publish to Unity\n        auto data_msg = std_msgs::msg::String();\n        data_msg.data = unity_data;\n        unity_data_pub_->publish(data_msg);\n    }\n\n    void cmdVelCallback(const geometry_msgs::msg::Twist::SharedPtr msg)\n    {\n        std::lock_guard<std::mutex> lock(velocity_mutex_);\n\n        // Store velocity commands\n        linear_velocity_ = msg->linear;\n        angular_velocity_ = msg->angular;\n\n        RCLCPP_DEBUG(this->get_logger(),\n                    "Received velocity command: linear=(%.2f, %.2f, %.2f), angular=(%.2f, %.2f, %.2f)",\n                    msg->linear.x, msg->linear.y, msg->linear.z,\n                    msg->angular.x, msg->angular.y, msg->angular.z);\n    }\n\n    std::string formatJointDataForUnity()\n    {\n        std::string data = "JOINT_STATES:";\n\n        for (size_t i = 0; i < joint_names_.size() && i < joint_positions_.size(); ++i) {\n            data += joint_names_[i] + ":" + std::to_string(joint_positions_[i]);\n            if (i < joint_names_.size() - 1) {\n                data += ";";\n            }\n        }\n\n        return data;\n    }\n\n    void unityCommunicationLoop()\n    {\n        rclcpp::Rate rate(publish_rate_);\n\n        while (rclcpp::ok()) {\n            // This would normally handle TCP communication with Unity\n            // For this example, we\'re using ROS topics to simulate the communication\n\n            rate.sleep();\n        }\n    }\n\n    // ROS components\n    rclcpp::Subscription<sensor_msgs::msg::JointState>::SharedPtr joint_state_sub_;\n    rclcpp::Subscription<geometry_msgs::msg::Twist>::SharedPtr cmd_vel_sub_;\n    rclcpp::Publisher<std_msgs::msg::String>::SharedPtr unity_data_pub_;\n\n    // Data storage\n    std::vector<std::string> joint_names_;\n    std::vector<double> joint_positions_;\n    geometry_msgs::msg::Vector3 linear_velocity_;\n    geometry_msgs::msg::Vector3 angular_velocity_;\n\n    // Thread and synchronization\n    std::thread unity_thread_;\n    std::mutex joint_state_mutex_;\n    std::mutex velocity_mutex_;\n\n    // Configuration\n    std::string robot_namespace_;\n    std::string unity_endpoint_;\n    double publish_rate_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<ROSBridgeManager>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,a.jsxs)(e.ol,{start:"2",children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.strong,{children:"Create a Unity TCP server simulation (in Python for demonstration):"})}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/scripts/unity_tcp_server.py"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport socket\nimport json\nimport threading\nimport time\nfrom std_msgs.msg import String\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom geometry_msgs.msg import Twist\n\nclass UnityTCPServer(Node):\n    def __init__(self):\n        super().__init__('unity_tcp_server')\n\n        # Declare parameters\n        self.declare_parameter('tcp_port', 10000)\n        self.declare_parameter('robot_namespace', '/simple_humanoid')\n\n        # Get parameters\n        self.tcp_port = self.get_parameter('tcp_port').value\n        self.robot_namespace = self.get_parameter('robot_namespace').value\n\n        # Create publishers\n        self.joint_state_pub = self.create_publisher(\n            JointState,\n            self.robot_namespace + '/joint_states',\n            10\n        )\n\n        self.cmd_vel_pub = self.create_publisher(\n            Twist,\n            self.robot_namespace + '/cmd_vel',\n            10\n        )\n\n        # Start TCP server in a separate thread\n        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n        server_thread = threading.Thread(target=self.start_server)\n        server_thread.daemon = True\n        server_thread.start()\n\n        self.get_logger().info(f'Unity TCP Server started on port {self.tcp_port}')\n\n    def start_server(self):\n        try:\n            self.server_socket.bind(('localhost', self.tcp_port))\n            self.server_socket.listen(5)\n            self.get_logger().info('Unity TCP Server listening...')\n\n            while rclpy.ok():\n                try:\n                    client_socket, address = self.server_socket.accept()\n                    self.get_logger().info(f'Connection from {address}')\n\n                    client_thread = threading.Thread(\n                        target=self.handle_client,\n                        args=(client_socket,)\n                    )\n                    client_thread.daemon = True\n                    client_thread.start()\n\n                except socket.error as e:\n                    self.get_logger().error(f'Socket error: {e}')\n                    break\n\n        except Exception as e:\n            self.get_logger().error(f'Server error: {e}')\n        finally:\n            self.server_socket.close()\n\n    def handle_client(self, client_socket):\n        try:\n            while rclpy.ok():\n                data = client_socket.recv(1024).decode('utf-8')\n                if not data:\n                    break\n\n                self.get_logger().debug(f'Received from Unity: {data}')\n\n                # Parse the message and handle accordingly\n                if data.startswith('JOINT_STATES:'):\n                    self.handle_joint_states(data)\n                elif data.startswith('CMD_VEL:'):\n                    self.handle_cmd_vel(data)\n                elif data.startswith('INTERACTION:'):\n                    self.handle_interaction(data)\n\n        except Exception as e:\n            self.get_logger().error(f'Client handler error: {e}')\n        finally:\n            client_socket.close()\n\n    def handle_joint_states(self, data):\n        # Parse joint state data from Unity\n        joint_data_str = data[len('JOINT_STATES:'):]\n        joint_pairs = joint_data_str.split(';')\n\n        joint_msg = JointState()\n        joint_msg.header.stamp = self.get_clock().now().to_msg()\n        joint_msg.header.frame_id = 'unity_robot'\n\n        for pair in joint_pairs:\n            if ':' in pair:\n                name, pos_str = pair.split(':', 1)\n                joint_msg.name.append(name)\n                joint_msg.position.append(float(pos_str))\n\n        self.joint_state_pub.publish(joint_msg)\n\n    def handle_cmd_vel(self, data):\n        # Parse velocity command from Unity\n        cmd_str = data[len('CMD_VEL:'):]\n        try:\n            cmd_data = json.loads(cmd_str)\n            twist_msg = Twist()\n            twist_msg.linear.x = cmd_data.get('linear_x', 0.0)\n            twist_msg.linear.y = cmd_data.get('linear_y', 0.0)\n            twist_msg.linear.z = cmd_data.get('linear_z', 0.0)\n            twist_msg.angular.x = cmd_data.get('angular_x', 0.0)\n            twist_msg.angular.y = cmd_data.get('angular_y', 0.0)\n            twist_msg.angular.z = cmd_data.get('angular_z', 0.0)\n\n            self.cmd_vel_pub.publish(twist_msg)\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in cmd_vel: {cmd_str}')\n\n    def handle_interaction(self, data):\n        # Handle interaction commands from Unity\n        interaction_data = data[len('INTERACTION:'):]\n        self.get_logger().info(f'Interaction command: {interaction_data}')\n\n        # Publish interaction as a string message\n        interaction_msg = String()\n        interaction_msg.data = interaction_data\n        # You would create a custom publisher for interactions if needed\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    server = UnityTCPServer()\n\n    try:\n        rclpy.spin(server)\n    except KeyboardInterrupt:\n        server.get_logger().info('Shutting down Unity TCP Server')\n    finally:\n        server.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsxs)(e.ol,{start:"3",children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.strong,{children:"Create a launch file for the Unity bridge:"})}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:["Create ",(0,a.jsx)(e.code,{children:"~/ros2_ws/src/unity_hri_simulation/launch/unity_bridge.launch.py"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, ExecuteProcess, RegisterEventHandler\nfrom launch.event_handlers import OnProcessExit\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Declare launch arguments\n    tcp_port_arg = DeclareLaunchArgument(\n        'tcp_port',\n        default_value='10000',\n        description='TCP port for Unity communication'\n    )\n\n    robot_namespace_arg = DeclareLaunchArgument(\n        'robot_namespace',\n        default_value='/simple_humanoid',\n        description='Robot namespace for topics'\n    )\n\n    # Get launch configurations\n    tcp_port = LaunchConfiguration('tcp_port')\n    robot_namespace = LaunchConfiguration('robot_namespace')\n\n    # Start the Unity TCP server\n    unity_tcp_server = Node(\n        package='unity_hri_simulation',\n        executable='unity_tcp_server',\n        name='unity_tcp_server',\n        parameters=[\n            {'tcp_port': tcp_port},\n            {'robot_namespace': robot_namespace}\n        ],\n        output='screen'\n    )\n\n    # Start the ROS bridge manager\n    ros_bridge_manager = Node(\n        package='unity_hri_simulation',\n        executable='ros_bridge_manager',\n        name='ros_bridge_manager',\n        parameters=[\n            {'robot_namespace': robot_namespace},\n            {'unity_endpoint': '127.0.0.1:' + tcp_port},\n            {'publish_rate': 30.0}\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        tcp_port_arg,\n        robot_namespace_arg,\n        unity_tcp_server,\n        ros_bridge_manager\n    ])\n"})}),"\n",(0,a.jsx)(e.h3,{id:"understanding-the-implementation",children:"Understanding the Implementation"}),"\n",(0,a.jsx)(e.p,{children:"The glass-box view reveals:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"The Unity-ROS bridge uses TCP sockets for real-time communication between Unity and ROS"}),"\n",(0,a.jsx)(e.li,{children:"The system handles bidirectional communication: ROS commands control Unity visualization, and Unity interactions can send commands back to ROS"}),"\n",(0,a.jsx)(e.li,{children:"The implementation includes proper synchronization and data formatting for efficient communication"}),"\n",(0,a.jsx)(e.li,{children:"The system is designed to handle real-time performance requirements for smooth HRI experiences"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"tiered-assessments",children:"Tiered Assessments"}),"\n",(0,a.jsx)(e.h3,{id:"tier-1-basic-understanding",children:"Tier 1: Basic Understanding"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"What is the Unity Robotics Hub and what components does it include?"}),"\n",(0,a.jsx)(e.li,{children:"Name three rendering techniques that improve visual fidelity in Unity."}),"\n",(0,a.jsx)(e.li,{children:"What is the purpose of the URDF Importer in Unity?"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"tier-2-application",children:"Tier 2: Application"}),"\n",(0,a.jsxs)(e.ol,{start:"4",children:["\n",(0,a.jsx)(e.li,{children:"Create a Unity scene with realistic lighting and materials for a humanoid robot in an indoor environment."}),"\n",(0,a.jsx)(e.li,{children:"Implement a basic Unity-ROS bridge that visualizes joint positions received from ROS."}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"tier-3-analysis-and-synthesis",children:"Tier 3: Analysis and Synthesis"}),"\n",(0,a.jsxs)(e.ol,{start:"6",children:["\n",(0,a.jsx)(e.li,{children:"Design a complete HRI system that integrates Unity visualization with ROS control, including safety monitoring, user feedback mechanisms, and performance optimization for real-time interaction."}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"mermaid-diagram",children:"Mermaid Diagram"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-mermaid",children:"graph TB\n    A[Human User] --\x3e B[Unity HRI Interface]\n    A --\x3e C[Voice Input]\n    A --\x3e D[Gestures]\n    A --\x3e E[Touch Interaction]\n\n    B --\x3e F[3D Visualization]\n    B --\x3e G[Audio Feedback]\n    B --\x3e H[Haptic Feedback]\n\n    I[Unity Engine] --\x3e J[Real-time Rendering]\n    I --\x3e K[Physics Simulation]\n    I --\x3e L[Material System]\n\n    M[ROS System] --\x3e N[Robot Controllers]\n    M --\x3e O[Sensors]\n    M --\x3e P[Navigation]\n\n    Q[Unity-ROS Bridge] --\x3e R[TCP Communication]\n    Q --\x3e S[Message Translation]\n    Q --\x3e T[Real-time Sync]\n\n    F --\x3e Q\n    J --\x3e I\n    N --\x3e M\n    Q --\x3e M\n\n    style A fill:#ff9999\n    style I fill:#99ccff\n    style M fill:#99ff99\n    style Q fill:#ffcc99\n"})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Alt-text for diagram:"}),' "High-fidelity HRI system architecture showing a human user interacting with a Unity HRI interface through voice input, gestures, and touch interaction. The interface provides 3D visualization, audio feedback, and haptic feedback. Unity engine handles real-time rendering, physics simulation, and material system. ROS system manages robot controllers, sensors, and navigation. A Unity-ROS bridge connects both systems via TCP communication, message translation, and real-time synchronization. The human user is highlighted in pink, Unity engine in light blue, ROS system in light green, and the bridge in light orange."']}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"This chapter explored the use of Unity for creating high-fidelity human-robot interaction environments. We covered the theoretical foundations of Unity's rendering capabilities, practical implementation of HRI interfaces, and the technical details of integrating Unity with ROS systems. Through examples, we demonstrated how to create realistic visualizations, implement interaction systems, and establish communication bridges between Unity and ROS for real-time robot control and visualization."}),"\n",(0,a.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Unity Technologies. (2022). Unity Robotics Hub: Documentation and tutorials. Unity Technologies."}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Unity Technologies. (2021). High Definition Render Pipeline User Guide. Unity Technologies."}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:"Siciliano, B., & Khatib, O. (2016). Springer handbook of robotics. Springer Publishing Company, Incorporated."}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Feil-Seifer, D., & Matari\u0107, M. J. (2009). Defining socially assistive robotics. ",(0,a.jsx)(e.em,{children:"2009 IEEE International Conference on Rehabilitation Robotics"}),", 1-6."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Broadbent, E., Stafford, R., & MacDonald, B. (2009). Acceptance of healthcare robots for the older population: Review and future directions. ",(0,a.jsx)(e.em,{children:"Annual Review of Biomedical Engineering"}),", 11, 385-406."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Kidd, C. D., & Breazeal, C. (2008). Robot helpers in the home: Features and preferences. ",(0,a.jsx)(e.em,{children:"2008 3rd ACM/IEEE International Conference on Human-Robot Interaction (HRI)"}),", 375-382."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Mubin, O., Stevens, C. J., Shahid, S., Al Mahmud, A., & Dong, J. J. (2013). A review of the applicability of robots in education. ",(0,a.jsx)(e.em,{children:"Journal of Technology in Education and Learning"}),", 1(1), 1-7."]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:["Dautenhahn, K. (2007). Socially assistive robots in elderly care and rehabilitation: A brief overview. ",(0,a.jsx)(e.em,{children:"Proceedings of the 9th International Conference on Rehabilitation Robotics"}),", 52-54."]}),"\n"]}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>o});var t=i(6540);const a={},r=t.createContext(a);function s(n){const e=t.useContext(r);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(r.Provider,{value:e},n.children)}}}]);