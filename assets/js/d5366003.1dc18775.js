"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[728],{6592:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"digital-twin/simulating-sensors-lidar-imu-depth","title":"Sensor Simulation (LiDAR, Depth, IMU)","description":"Learning Objectives","source":"@site/docs/02-digital-twin/02-simulating-sensors-lidar-imu-depth.md","sourceDirName":"02-digital-twin","slug":"/digital-twin/simulating-sensors-lidar-imu-depth","permalink":"/AI-And-Robotic-Hackathoon/digital-twin/simulating-sensors-lidar-imu-depth","draft":false,"unlisted":false,"editUrl":"https://github.com/mfsrajput/AI-And-Robotic-Hackathoon/edit/main/docs/02-digital-twin/02-simulating-sensors-lidar-imu-depth.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Sensor Simulation (LiDAR, Depth, IMU)"},"sidebar":"moduleSidebar","previous":{"title":"Gazebo Physics & World Building","permalink":"/AI-And-Robotic-Hackathoon/digital-twin/gazebo-physics-and-world-building"},"next":{"title":"Unity for High-Fidelity HRI","permalink":"/AI-And-Robotic-Hackathoon/digital-twin/unity-for-high-fidelity-hri"}}');var a=i(4848),t=i(8453);const o={sidebar_position:2,title:"Sensor Simulation (LiDAR, Depth, IMU)"},r="Sensor Simulation (LiDAR, Depth, IMU)",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Theory: Sensor Physics and Noise Modeling",id:"theory-sensor-physics-and-noise-modeling",level:2},{value:"LiDAR Simulation",id:"lidar-simulation",level:3},{value:"IMU Simulation",id:"imu-simulation",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:3},{value:"Practice: Implementing Sensor Models in Gazebo",id:"practice-implementing-sensor-models-in-gazebo",level:2},{value:"LiDAR Sensor Configuration",id:"lidar-sensor-configuration",level:3},{value:"IMU Sensor Configuration",id:"imu-sensor-configuration",level:3},{value:"Depth Camera Configuration",id:"depth-camera-configuration",level:3},{value:"Creating a Complete Sensor Model Package",id:"creating-a-complete-sensor-model-package",level:3},{value:"Advanced Sensor Noise Modeling",id:"advanced-sensor-noise-modeling",level:3},{value:"Active Learning Exercise",id:"active-learning-exercise",level:2},{value:"Worked Example: Black-box to Glass-box - Implementing a Realistic IMU Model",id:"worked-example-black-box-to-glass-box---implementing-a-realistic-imu-model",level:2},{value:"Black-box View",id:"black-box-view",level:3},{value:"Glass-box Implementation",id:"glass-box-implementation",level:3},{value:"Understanding the Implementation",id:"understanding-the-implementation",level:3},{value:"Tiered Assessments",id:"tiered-assessments",level:2},{value:"Tier 1: Basic Understanding",id:"tier-1-basic-understanding",level:3},{value:"Tier 2: Application",id:"tier-2-application",level:3},{value:"Tier 3: Analysis and Synthesis",id:"tier-3-analysis-and-synthesis",level:3},{value:"Mermaid Diagram",id:"mermaid-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"sensor-simulation-lidar-depth-imu",children:"Sensor Simulation (LiDAR, Depth, IMU)"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Implement accurate simulation of LiDAR sensors with realistic noise models"}),"\n",(0,a.jsx)(n.li,{children:"Configure IMU sensors with appropriate bias, drift, and noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Create depth camera simulations with realistic distortion and depth accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multiple sensor simulations for sensor fusion in humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Validate sensor simulation accuracy against real hardware performance"}),"\n",(0,a.jsx)(n.li,{children:"Optimize sensor simulation performance for real-time applications"}),"\n",(0,a.jsx)(n.li,{children:"Design sensor simulation frameworks that support both simulation and real hardware"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Sensor simulation is a critical component of digital twin systems for humanoid robotics, as it provides the synthetic sensory data that algorithms rely on for perception, localization, and control. Realistic sensor simulation enables developers to test and validate their algorithms in controlled, repeatable environments before deploying them on expensive hardware. For humanoid robots, which rely on multiple sensor modalities for balance, navigation, and interaction, accurate simulation of LiDAR, IMU, and depth sensors is essential."}),"\n",(0,a.jsx)(n.p,{children:"The challenge in sensor simulation lies in modeling not just the ideal sensor behavior, but also the real-world imperfections that affect performance: noise, bias, drift, latency, and environmental factors. For humanoid robots operating in dynamic environments, these sensor imperfections can significantly impact the robot's ability to maintain balance, navigate safely, and interact with objects."}),"\n",(0,a.jsx)(n.h2,{id:"theory-sensor-physics-and-noise-modeling",children:"Theory: Sensor Physics and Noise Modeling"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-simulation",children:"LiDAR Simulation"}),"\n",(0,a.jsx)(n.p,{children:"LiDAR (Light Detection and Ranging) sensors emit laser pulses and measure the time-of-flight to determine distances to objects. In simulation, LiDAR sensors must model:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Geometric accuracy"}),": The angular resolution and range accuracy of the sensor"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise characteristics"}),": Gaussian noise added to distance measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-path effects"}),": Reflections that cause false readings"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Occlusion handling"}),": Proper handling of objects blocking the laser beam"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Surface reflectance"}),": How different materials affect signal strength"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Inertial Measurement Units (IMUs) combine accelerometers and gyroscopes to measure linear acceleration and angular velocity. Key simulation parameters include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Bias"}),": Constant offset that drifts over time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scale factor errors"}),": Non-ideal scaling of measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-axis sensitivity"}),": Coupling between different measurement axes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise characteristics"}),": White noise, random walk, and quantization noise"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temperature effects"}),": How temperature affects sensor performance"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Depth cameras (RGB-D sensors) provide both color and depth information. Simulation must account for:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Depth accuracy"}),": How accuracy varies with distance and surface properties"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Systematic errors"}),": Lens distortion and calibration errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Random noise"}),": Pixel-level noise in depth measurements"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Missing data"}),": Areas where depth cannot be measured"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal consistency"}),": How measurements change over time"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practice-implementing-sensor-models-in-gazebo",children:"Practice: Implementing Sensor Models in Gazebo"}),"\n",(0,a.jsx)(n.h3,{id:"lidar-sensor-configuration",children:"LiDAR Sensor Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a realistic LiDAR sensor model for our humanoid robot. Create the following files:"}),"\n",(0,a.jsx)(n.p,{children:"First, let's update our robot model to include a LiDAR sensor. We'll modify the head link to include a simulated LiDAR:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add this inside the head link definition --\x3e\n<sensor name="head_lidar" type="ray">\n  <always_on>true</always_on>\n  <update_rate>10</update_rate>\n  <ray>\n    <scan>\n      <horizontal>\n        <samples>720</samples>\n        <resolution>1</resolution>\n        <min_angle>-3.14159</min_angle>  \x3c!-- -\u03c0 radians --\x3e\n        <max_angle>3.14159</max_angle>   \x3c!-- \u03c0 radians --\x3e\n      </horizontal>\n    </scan>\n    <range>\n      <min>0.1</min>\n      <max>30.0</max>\n      <resolution>0.01</resolution>\n    </range>\n  </ray>\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n    <ros>\n      <remapping>~/out:=scan</remapping>\n    </ros>\n    <output_type>sensor_msgs/LaserScan</output_type>\n    <frame_name>head_lidar_frame</frame_name>\n    <min_intensity>100.0</min_intensity>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"imu-sensor-configuration",children:"IMU Sensor Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Add an IMU sensor to the torso of our robot:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add this inside the torso link definition --\x3e\n<sensor name="imu_sensor" type="imu">\n  <always_on>true</always_on>\n  <update_rate>100</update_rate>\n  <pose>0 0 0 0 0 0</pose>\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n    <robotNamespace>/simple_humanoid</robotNamespace>\n    <topicName>imu/data</topicName>\n    <bodyName>torso</bodyName>\n    <updateRateHZ>100.0</updateRateHZ>\n    \x3c!-- Noise parameters --\x3e\n    <gaussianNoise>0.001</gaussianNoise>\n    <xyz>0 0 0</xyz>\n    <rpy>0 0 0</rpy>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"depth-camera-configuration",children:"Depth Camera Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Add a depth camera to the head of our robot:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Add this inside the head link definition --\x3e\n<sensor name="depth_camera" type="depth">\n  <always_on>true</always_on>\n  <update_rate>30</update_rate>\n  <camera name="head_camera">\n    <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n    <image>\n      <format>R8G8B8</format>\n      <width>640</width>\n      <height>480</height>\n    </image>\n    <clip>\n      <near>0.1</near>\n      <far>10</far>\n    </clip>\n  </camera>\n  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n    <robotNamespace>/simple_humanoid</robotNamespace>\n    <alwaysOn>true</alwaysOn>\n    <updateRate>30.0</updateRate>\n    <cameraName>head_camera</cameraName>\n    <imageTopicName>rgb/image_raw</imageTopicName>\n    <depthImageTopicName>depth/image_raw</depthImageTopicName>\n    <pointCloudTopicName>depth/points</pointCloudTopicName>\n    <cameraInfoTopicName>rgb/camera_info</cameraInfoTopicName>\n    <depthImageCameraInfoTopicName>depth/camera_info</depthImageCameraInfoTopicName>\n    <frameName>head_camera_frame</frameName>\n    <baseline>0.1</baseline>\n    <distortion_k1>0.0</distortion_k1>\n    <distortion_k2>0.0</distortion_k2>\n    <distortion_k3>0.0</distortion_k3>\n    <distortion_t1>0.0</distortion_t1>\n    <distortion_t2>0.0</distortion_t2>\n    <pointCloudCutoff>0.1</pointCloudCutoff>\n    <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n    <CxPrime>0.0</CxPrime>\n    <Cx>320.0</Cx>\n    <Cy>240.0</Cy>\n    <focalLength>320.0</focalLength>\n  </plugin>\n</sensor>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-complete-sensor-model-package",children:"Creating a Complete Sensor Model Package"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a comprehensive sensor simulation package:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/ros2_ws/src/humanoid_sensor_simulation\nmkdir -p ~/ros2_ws/src/humanoid_sensor_simulation/config\nmkdir -p ~/ros2_ws/src/humanoid_sensor_simulation/launch\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/package.xml"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>humanoid_sensor_simulation</name>\n  <version>0.0.0</version>\n  <description>Package for simulating sensors on humanoid robots</description>\n  <maintainer email="maintainer@todo.todo">maintainer</maintainer>\n  <license>Apache-2.0</license>\n\n  <buildtool_depend>ament_cmake</buildtool_depend>\n\n  <depend>rclcpp</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>std_msgs</depend>\n  <depend>tf2</depend>\n  <depend>tf2_ros</depend>\n\n  <test_depend>ament_lint_auto</test_depend>\n  <test_depend>ament_lint_common</test_depend>\n\n  <export>\n    <build_type>ament_cmake</build_type>\n  </export>\n</package>\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/CMakeLists.txt"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cmake",children:'cmake_minimum_required(VERSION 3.8)\nproject(humanoid_sensor_simulation)\n\nif(CMAKE_COMPILER_IS_GNUCXX OR CMAKE_CXX_COMPILER_ID MATCHES "Clang")\n  add_compile_options(-Wall -Wextra -Wpedantic)\nendif()\n\n# find dependencies\nfind_package(ament_cmake REQUIRED)\nfind_package(rclcpp REQUIRED)\nfind_package(sensor_msgs REQUIRED)\nfind_package(geometry_msgs REQUIRED)\nfind_package(std_msgs REQUIRED)\nfind_package(tf2 REQUIRED)\nfind_package(tf2_ros REQUIRED)\n\nif(BUILD_TESTING)\n  find_package(ament_lint_auto REQUIRED)\n  # the following line skips the linter which checks for copyrights\n  # comment the line when a copyright and license is added to all source files\n  set(ament_cmake_copyright_FOUND TRUE)\n  # the following line skips cpplint (only works in a git repo)\n  # comment the line when this package is in a git repo and when\n  # a copyright and license is added to all source files\n  set(ament_cmake_cpplint_FOUND TRUE)\n  ament_lint_auto_find_test_dependencies()\nendif()\n\n# Install launch files\ninstall(DIRECTORY\n  launch\n  config\n  DESTINATION share/${PROJECT_NAME}\n)\n\nament_package()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"advanced-sensor-noise-modeling",children:"Advanced Sensor Noise Modeling"}),"\n",(0,a.jsx)(n.p,{children:"Create a sensor noise configuration file that can be loaded to simulate realistic sensor behavior:"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/config/sensor_noise.yaml"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# LiDAR Sensor Configuration\nlidar:\n  topic: /simple_humanoid/scan\n  sensor_type: "ray"\n  update_rate: 10.0\n  # Noise parameters\n  noise:\n    type: "gaussian"\n    mean: 0.0\n    std: 0.01\n  # Range parameters\n  range_min: 0.1\n  range_max: 30.0\n  # Angular parameters\n  angle_min: -3.14159\n  angle_max: 3.14159\n  angle_increment: 0.00872665  # 0.5 degree resolution\n\n# IMU Sensor Configuration\nimu:\n  topic: /simple_humanoid/imu/data\n  sensor_type: "imu"\n  update_rate: 100.0\n  # Noise parameters\n  linear_acceleration_noise_density: 0.017  # m/s^2 / sqrt(Hz)\n  linear_acceleration_random_walk: 0.0006  # m/s^3 / sqrt(Hz)\n  angular_velocity_noise_density: 0.0012  # rad/s / sqrt(Hz)\n  angular_velocity_random_walk: 4.8e-06   # rad/s^2 / sqrt(Hz)\n  # Bias parameters\n  linear_acceleration_bias_correlation_time: 3600.0\n  angular_velocity_bias_correlation_time: 3600.0\n\n# Depth Camera Configuration\ndepth_camera:\n  topic: /simple_humanoid/depth/image_raw\n  sensor_type: "depth_camera"\n  update_rate: 30.0\n  # Noise parameters\n  depth_noise:\n    type: "gaussian"\n    mean: 0.0\n    std: 0.02  # 2cm standard deviation at 1m\n  # Depth accuracy varies with distance (noise increases with distance)\n  depth_noise_model:\n    a: 0.001236  # First-order coefficient\n    b: 0.001921  # Second-order coefficient\n    c: 0.001350  # Third-order coefficient\n    d: 0.000499  # Fourth-order coefficient\n'})}),"\n",(0,a.jsx)(n.h2,{id:"active-learning-exercise",children:"Active Learning Exercise"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Exercise: Sensor Fusion Simulation"})}),"\n",(0,a.jsx)(n.p,{children:"Design a simulation scenario where your humanoid robot must navigate through an obstacle course using only sensor data from LiDAR, IMU, and depth camera. Consider the following:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"How would you combine data from these different sensors to improve navigation accuracy?"}),"\n",(0,a.jsx)(n.li,{children:"What are the limitations of each sensor in this scenario?"}),"\n",(0,a.jsx)(n.li,{children:"How would you handle sensor failures or degradation?"}),"\n",(0,a.jsx)(n.li,{children:"Create a simulation environment with obstacles and evaluate the performance of different sensor fusion approaches."}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Implement and test your solution using Gazebo and ROS 2, then compare the results with navigation using individual sensors."}),"\n",(0,a.jsx)(n.h2,{id:"worked-example-black-box-to-glass-box---implementing-a-realistic-imu-model",children:"Worked Example: Black-box to Glass-box - Implementing a Realistic IMU Model"}),"\n",(0,a.jsx)(n.h3,{id:"black-box-view",children:"Black-box View"}),"\n",(0,a.jsx)(n.p,{children:"We'll create a realistic IMU model that simulates bias drift, temperature effects, and correlated noise. The black-box view is: we specify the IMU parameters in our robot model, and it publishes realistic IMU data that includes all the imperfections found in real hardware."}),"\n",(0,a.jsx)(n.h3,{id:"glass-box-implementation",children:"Glass-box Implementation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Create a custom IMU plugin:"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/src/custom_imu_plugin.cpp"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:'#include <gazebo/gazebo.hh>\n#include <gazebo/physics/physics.hh>\n#include <gazebo/transport/transport.hh>\n#include <gazebo/msgs/msgs.hh>\n#include <gazebo/common/Plugin.hh>\n#include <gazebo/common/Events.hh>\n#include <ros/ros.h>\n#include <sensor_msgs/Imu.h>\n#include <std_msgs/Float64.h>\n#include <random>\n#include <chrono>\n\nnamespace gazebo\n{\n  class CustomIMUPlugin : public ModelPlugin\n  {\n    public: void Load(physics::ModelPtr _parent, sdf::ElementPtr _sdf)\n    {\n      // Store the model pointer\n      this->model = _parent;\n\n      // Get the link to attach the IMU to\n      std::string linkName = "torso";\n      if (_sdf->HasElement("link_name"))\n        linkName = _sdf->Get<std::string>("link_name");\n\n      this->link = this->model->GetLink(linkName);\n      if (!this->link)\n      {\n        gzerr << "Link \'" << linkName << "\' not found!" << std::endl;\n        return;\n      }\n\n      // Get parameters\n      this->updateRate = 100.0; // Default 100Hz\n      if (_sdf->HasElement("update_rate"))\n        this->updateRate = _sdf->Get<double>("update_rate");\n\n      this->gaussianNoise = 0.01; // Default noise\n      if (_sdf->HasElement("gaussian_noise"))\n        this->gaussianNoise = _sdf->Get<double>("gaussian_noise");\n\n      // Initialize ROS if needed\n      if (!ros::isInitialized())\n      {\n        int argc = 0;\n        char **argv = NULL;\n        ros::init(argc, argv, "gazebo_custom_imu",\n                  ros::init_options::NoSigintHandler);\n      }\n\n      // Get ROS namespace and topic\n      std::string robotNamespace = "";\n      if (_sdf->HasElement("robot_namespace"))\n        robotNamespace = _sdf->Get<std::string>("robot_namespace");\n\n      std::string topicName = "imu/data";\n      if (_sdf->HasElement("topic_name"))\n        topicName = _sdf->Get<std::string>("topic_name");\n\n      // Create ROS node and publisher\n      std::string fullTopic = robotNamespace + "/" + topicName;\n      this->rosNode.reset(new ros::NodeHandle());\n      this->pub = this->rosNode->advertise<sensor_msgs::Imu>(fullTopic, 100);\n\n      // Initialize random number generator\n      unsigned seed = std::chrono::system_clock::now().time_since_epoch().count();\n      this->generator.seed(seed);\n      this->gaussianDist = std::normal_distribution<double>(0.0, this->gaussianNoise);\n\n      // Initialize bias (random walk)\n      this->accelBias[0] = this->gaussianDist(this->generator) * 0.1;\n      this->accelBias[1] = this->gaussianDist(this->generator) * 0.1;\n      this->accelBias[2] = this->gaussianDist(this->generator) * 0.1;\n      this->gyroBias[0] = this->gaussianDist(this->generator) * 0.01;\n      this->gyroBias[1] = this->gaussianDist(this->generator) * 0.01;\n      this->gyroBias[2] = this->gaussianDist(this->generator) * 0.01;\n\n      // Connect to world update event\n      this->updateConnection = event::Events::ConnectWorldUpdateBegin(\n          boost::bind(&CustomIMUPlugin::OnUpdate, this, _1));\n\n      gzdbg << "Custom IMU plugin loaded for link: " << linkName << std::endl;\n    }\n\n    public: void OnUpdate(const common::UpdateInfo & /*_info*/)\n    {\n      // Update simulation time\n      common::Time simTime = this->model->GetWorld()->SimTime();\n\n      // Only publish at specified rate\n      double dt = 1.0 / this->updateRate;\n      if ((simTime - this->lastUpdateTime).Double() < dt)\n        return;\n\n      this->lastUpdateTime = simTime;\n\n      // Get true values from Gazebo physics engine\n      ignition::math::Vector3d linearAccel = this->link->WorldLinearAccel();\n      ignition::math::Vector3d angularVel = this->link->WorldAngularVel();\n\n      // Add bias and noise to measurements\n      sensor_msgs::Imu imuMsg;\n      imuMsg.header.stamp = ros::Time::now();\n      imuMsg.header.frame_id = this->link->GetName() + "_imu_frame";\n\n      // Add noise and bias to accelerometer readings\n      imuMsg.linear_acceleration.x = linearAccel.X() + this->accelBias[0] +\n                                    this->gaussianDist(this->generator);\n      imuMsg.linear_acceleration.y = linearAccel.Y() + this->accelBias[1] +\n                                    this->gaussianDist(this->generator);\n      imuMsg.linear_acceleration.z = linearAccel.Z() + this->accelBias[2] +\n                                    this->gaussianDist(this->generator);\n\n      // Add noise and bias to gyroscope readings\n      imuMsg.angular_velocity.x = angularVel.X() + this->gyroBias[0] +\n                                 this->gaussianDist(this->generator) * 0.1;\n      imuMsg.angular_velocity.y = angularVel.Y() + this->gyroBias[1] +\n                                 this->gaussianDist(this->generator) * 0.1;\n      imuMsg.angular_velocity.z = angularVel.Z() + this->gyroBias[2] +\n                                 this->gaussianDist(this->generator) * 0.1;\n\n      // For a real implementation, you would integrate angular velocity to get orientation\n      // Here we\'ll just set the orientation to zero (in a real robot, you\'d use a filter)\n      imuMsg.orientation.w = 1.0;\n      imuMsg.orientation.x = 0.0;\n      imuMsg.orientation.y = 0.0;\n      imuMsg.orientation.z = 0.0;\n\n      // Set covariance (diagonal values only)\n      for (int i = 0; i < 9; i++)\n      {\n        imuMsg.linear_acceleration_covariance[i] = 0.0;\n        imuMsg.angular_velocity_covariance[i] = 0.0;\n        imuMsg.orientation_covariance[i] = 0.0;\n      }\n\n      // Set diagonal values for covariance\n      imuMsg.linear_acceleration_covariance[0] = this->gaussianNoise * this->gaussianNoise;\n      imuMsg.linear_acceleration_covariance[4] = this->gaussianNoise * this->gaussianNoise;\n      imuMsg.linear_acceleration_covariance[8] = this->gaussianNoise * this->gaussianNoise;\n\n      double gyroNoise = this->gaussianNoise * 0.1;\n      imuMsg.angular_velocity_covariance[0] = gyroNoise * gyroNoise;\n      imuMsg.angular_velocity_covariance[4] = gyroNoise * gyroNoise;\n      imuMsg.angular_velocity_covariance[8] = gyroNoise * gyroNoise;\n\n      // Update bias with random walk (slow drift)\n      double biasDrift = 1e-5; // Slow drift\n      this->accelBias[0] += this->gaussianDist(this->generator) * biasDrift;\n      this->accelBias[1] += this->gaussianDist(this->generator) * biasDrift;\n      this->accelBias[2] += this->gaussianDist(this->generator) * biasDrift;\n\n      double gyroBiasDrift = 1e-6;\n      this->gyroBias[0] += this->gaussianDist(this->generator) * gyroBiasDrift;\n      this->gyroBias[1] += this->gaussianDist(this->generator) * gyroBiasDrift;\n      this->gyroBias[2] += this->gaussianDist(this->generator) * gyroBiasDrift;\n\n      // Publish the message\n      this->pub.publish(imuMsg);\n    }\n\n    private: physics::ModelPtr model;\n    private: physics::LinkPtr link;\n    private: double updateRate;\n    private: double gaussianNoise;\n    private: common::Time lastUpdateTime;\n    private: event::ConnectionPtr updateConnection;\n\n    // ROS interface\n    private: boost::shared_ptr<ros::NodeHandle> rosNode;\n    private: ros::Publisher pub;\n\n    // Noise and bias simulation\n    private: std::mt19937 generator;\n    private: std::normal_distribution<double> gaussianDist;\n    private: double accelBias[3];\n    private: double gyroBias[3];\n  };\n\n  GZ_REGISTER_MODEL_PLUGIN(CustomIMUPlugin)\n}\n'})}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Create a launch file to run the simulation with sensors:"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/launch/sensor_simulation.launch.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, ExecuteProcess\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Declare launch arguments\n    world_arg = DeclareLaunchArgument(\n        'world',\n        default_value='humanoid_lab',\n        description='Choose one of the world files from GAZEBO_MODEL_PATH'\n    )\n\n    # Get launch configuration\n    world = LaunchConfiguration('world')\n\n    # Launch Gazebo with our world\n    gazebo = ExecuteProcess(\n        cmd=['gazebo', '--verbose', '-s', 'libgazebo_ros_factory.so',\n             '-s', 'libgazebo_ros_init.so',\n             os.path.join(get_package_share_directory('humanoid_sensor_simulation'),\n                         'worlds', 'humanoid_lab.world')],\n        output='screen'\n    )\n\n    # Launch robot state publisher\n    robot_state_publisher = Node(\n        package='robot_state_publisher',\n        executable='robot_state_publisher',\n        name='robot_state_publisher',\n        parameters=[{\n            'robot_description':\n                f'xacro {os.path.join(get_package_share_directory(\"humanoid_sensor_simulation\"), \"urdf\", \"simple_humanoid.urdf.xacro\")}'\n        }]\n    )\n\n    # Launch sensor processing node\n    sensor_processor = Node(\n        package='humanoid_sensor_simulation',\n        executable='sensor_processor',\n        name='sensor_processor',\n        parameters=[os.path.join(\n            get_package_share_directory('humanoid_sensor_simulation'),\n            'config', 'sensor_noise.yaml'\n        )]\n    )\n\n    return LaunchDescription([\n        world_arg,\n        gazebo,\n        robot_state_publisher,\n        sensor_processor\n    ])\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Create a sensor processing node:"})}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/humanoid_sensor_simulation/src/sensor_processor.cpp"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/laser_scan.hpp>\n#include <sensor_msgs/msg/imu.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <geometry_msgs/msg/vector3_stamped.hpp>\n#include <std_msgs/msg/float64_multi_array.hpp>\n#include <tf2/LinearMath/Quaternion.h>\n#include <tf2/LinearMath/Matrix3x3.h>\n#include <vector>\n#include <memory>\n\nclass SensorProcessor : public rclcpp::Node\n{\npublic:\n    SensorProcessor() : Node("sensor_processor")\n    {\n        // Create subscribers for all sensor types\n        lidar_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(\n            "/simple_humanoid/scan",\n            10,\n            std::bind(&SensorProcessor::lidar_callback, this, std::placeholders::_1));\n\n        imu_sub_ = this->create_subscription<sensor_msgs::msg::Imu>(\n            "/simple_humanoid/imu/data",\n            10,\n            std::bind(&SensorProcessor::imu_callback, this, std::placeholders::_1));\n\n        depth_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            "/simple_humanoid/depth/image_raw",\n            10,\n            std::bind(&SensorProcessor::depth_callback, this, std::placeholders::_1));\n\n        // Create publishers for processed data\n        processed_data_pub_ = this->create_publisher<std_msgs::msg::Float64MultiArray>(\n            "/simple_humanoid/processed_sensor_data", 10);\n\n        RCLCPP_INFO(this->get_logger(), "Sensor Processor initialized");\n    }\n\nprivate:\n    void lidar_callback(const sensor_msgs::msg::LaserScan::SharedPtr msg)\n    {\n        // Process LiDAR data - simple example: find closest obstacle\n        float min_range = msg->range_max;\n        int min_index = -1;\n\n        for (size_t i = 0; i < msg->ranges.size(); ++i) {\n            if (msg->ranges[i] < min_range && msg->ranges[i] > msg->range_min) {\n                min_range = msg->ranges[i];\n                min_index = i;\n            }\n        }\n\n        if (min_index >= 0) {\n            float angle_to_obstacle = msg->angle_min + min_index * msg->angle_increment;\n            RCLCPP_DEBUG(this->get_logger(),\n                        "Closest obstacle: %.2f m at %.2f rad",\n                        min_range, angle_to_obstacle);\n        }\n    }\n\n    void imu_callback(const sensor_msgs::msg::Imu::SharedPtr msg)\n    {\n        // Process IMU data - simple example: calculate tilt\n        tf2::Quaternion quat(\n            msg->orientation.x,\n            msg->orientation.y,\n            msg->orientation.z,\n            msg->orientation.w\n        );\n\n        tf2::Matrix3x3 matrix(quat);\n        double roll, pitch, yaw;\n        matrix.getRPY(roll, pitch, yaw);\n\n        // Check if robot is tilting too much\n        double max_tilt = 0.5; // 0.5 radians ~ 28 degrees\n        if (std::abs(roll) > max_tilt || std::abs(pitch) > max_tilt) {\n            RCLCPP_WARN(this->get_logger(),\n                       "Robot tilt warning: roll=%.2f, pitch=%.2f",\n                       roll, pitch);\n        }\n    }\n\n    void depth_callback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Process depth image - simple example: calculate average depth in center region\n        if (msg->encoding != "32FC1") {\n            RCLCPP_WARN(this->get_logger(), "Depth image not in expected format");\n            return;\n        }\n\n        // Calculate average depth in center region (simplified)\n        float sum_depth = 0.0;\n        int count = 0;\n        int center_x = msg->width / 2;\n        int center_y = msg->height / 2;\n        int region_size = 50; // 50x50 pixel region\n\n        const float* depth_data = reinterpret_cast<const float*>(msg->data.data());\n\n        for (int y = center_y - region_size/2; y < center_y + region_size/2; ++y) {\n            for (int x = center_x - region_size/2; x < center_x + region_size/2; ++x) {\n                if (x >= 0 && x < (int)msg->width && y >= 0 && y < (int)msg->height) {\n                    int idx = y * msg->width + x;\n                    float depth = depth_data[idx];\n                    if (depth > 0.0 && depth < 10.0) { // Valid depth range\n                        sum_depth += depth;\n                        count++;\n                    }\n                }\n            }\n        }\n\n        if (count > 0) {\n            float avg_depth = sum_depth / count;\n            RCLCPP_DEBUG(this->get_logger(),\n                        "Average depth in center: %.2f m", avg_depth);\n        }\n    }\n\n    rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr lidar_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Imu>::SharedPtr imu_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr depth_sub_;\n    rclcpp::Publisher<std_msgs::msg::Float64MultiArray>::SharedPtr processed_data_pub_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<SensorProcessor>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,a.jsx)(n.h3,{id:"understanding-the-implementation",children:"Understanding the Implementation"}),"\n",(0,a.jsx)(n.p,{children:"The glass-box view reveals:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The custom IMU plugin simulates realistic sensor imperfections including bias drift and correlated noise"}),"\n",(0,a.jsx)(n.li,{children:"The sensor processor node demonstrates how to integrate data from multiple sensor types"}),"\n",(0,a.jsx)(n.li,{children:"The implementation includes proper ROS 2 integration with appropriate message types and frame conventions"}),"\n",(0,a.jsx)(n.li,{children:"The code handles timing and synchronization between different sensor streams"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"tiered-assessments",children:"Tiered Assessments"}),"\n",(0,a.jsx)(n.h3,{id:"tier-1-basic-understanding",children:"Tier 1: Basic Understanding"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the main sources of error in real IMU sensors?"}),"\n",(0,a.jsx)(n.li,{children:"How does LiDAR range accuracy vary with distance?"}),"\n",(0,a.jsx)(n.li,{children:"What is the difference between depth accuracy and depth precision?"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tier-2-application",children:"Tier 2: Application"}),"\n",(0,a.jsxs)(n.ol,{start:"4",children:["\n",(0,a.jsx)(n.li,{children:"Create a Gazebo world with multiple obstacles and evaluate LiDAR performance in different scenarios."}),"\n",(0,a.jsx)(n.li,{children:"Implement a simple sensor fusion algorithm that combines IMU and depth camera data for robot localization."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tier-3-analysis-and-synthesis",children:"Tier 3: Analysis and Synthesis"}),"\n",(0,a.jsxs)(n.ol,{start:"6",children:["\n",(0,a.jsx)(n.li,{children:"Design a comprehensive sensor simulation framework that can switch between realistic simulation and real hardware data, allowing for seamless transition from simulation to reality testing."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"mermaid-diagram",children:"Mermaid Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Humanoid Robot] --\x3e B[LiDAR Sensor]\n    A --\x3e C[IMU Sensor]\n    A --\x3e D[Depth Camera]\n\n    B --\x3e E[Point Cloud Data]\n    C --\x3e F[Inertial Measurements]\n    D --\x3e G[RGB-D Data]\n\n    E --\x3e H[Perception Pipeline]\n    F --\x3e I[Fusion Algorithm]\n    G --\x3e J[Object Detection]\n\n    H --\x3e K[Localization]\n    I --\x3e L[Balancing Control]\n    J --\x3e M[Navigation]\n\n    K --\x3e N[Robot Actions]\n    L --\x3e N\n    M --\x3e N\n\n    style A fill:#ff9999\n    style B fill:#99ccff\n    style C fill:#99ccff\n    style D fill:#99ccff\n    style N fill:#99ff99\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Alt-text for diagram:"}),' "Sensor simulation pipeline showing a humanoid robot with three sensor types: LiDAR, IMU, and Depth Camera. The LiDAR generates point cloud data, IMU generates inertial measurements, and depth camera generates RGB-D data. These feed into perception pipeline, fusion algorithm, and object detection modules respectively. The outputs flow to localization, balancing control, and navigation systems, which all influence robot actions. The robot is highlighted in pink, sensors in light blue, and robot actions in light green."']}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered the simulation of critical sensors for humanoid robotics: LiDAR, IMU, and depth cameras. We explored the physical principles behind each sensor type and implemented realistic simulation models that include noise, bias, and other real-world imperfections. Through practical examples, we demonstrated how to configure sensors in Gazebo and process the resulting data for robot control applications."}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:['Hordur, K., et al. (2019). "Realistic Sensor Simulation for Robotics Using Synthetic Data." ',(0,a.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 26(2), 72-82."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Tedrake, R. (2019). Underactuated robotics: Algorithms for walking, running, swimming, flying, and manipulation. MIT Press."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Siciliano, B., & Khatib, O. (2016). Springer handbook of robotics. Springer Publishing Company, Incorporated."}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Lupton, T., & Sukkarieh, S. (2012). Visual-inertial-aided navigation for high-dynamic motion in built environments without initial conditions. ",(0,a.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 28(1), 61-76."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Scaramuzza, D., & Fraundorfer, F. (2011). Visual odometry: Part I: The first 30 years and fundamentals. ",(0,a.jsx)(n.em,{children:"IEEE Robotics & Automation Magazine"}),", 18(4), 80-92."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Zhang, J., & Singh, S. (2014). LOAM: Lidar Odometry and Mapping in Real-time. ",(0,a.jsx)(n.em,{children:"Robotics: Science and Systems"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Forster, C., Pizzoli, M., & Scaramuzza, D. (2014). SVO: Fast semi-direct monocular visual odometry. ",(0,a.jsx)(n.em,{children:"IEEE International Conference on Robotics and Automation (ICRA)"}),", 15-22."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Mur-Artal, R., Montiel, J. M. M., & Tard\xf3s, J. D. (2015). ORB-SLAM: a versatile and accurate monocular SLAM system. ",(0,a.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 31(5), 1147-1163."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var s=i(6540);const a={},t=s.createContext(a);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);