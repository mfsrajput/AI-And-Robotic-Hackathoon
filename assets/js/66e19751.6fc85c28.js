"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[83],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var a=i(6540);const t={},s=a.createContext(t);function r(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},9422:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"nvidia-isaac/isaac-ros-vslam-perception","title":"Isaac ROS + Hardware-Accelerated VSLAM","description":"Learning Objectives","source":"@site/docs/03-nvidia-isaac/02-isaac-ros-vslam-perception.md","sourceDirName":"03-nvidia-isaac","slug":"/nvidia-isaac/isaac-ros-vslam-perception","permalink":"/AI-And-Robotic-Hackathoon/nvidia-isaac/isaac-ros-vslam-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/mfsrajput/AI-And-Robotic-Hackathoon/edit/main/docs/03-nvidia-isaac/02-isaac-ros-vslam-perception.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Isaac ROS + Hardware-Accelerated VSLAM"},"sidebar":"moduleSidebar","previous":{"title":"Isaac Sim & Synthetic Data Generation","permalink":"/AI-And-Robotic-Hackathoon/nvidia-isaac/isaac-sim-synthetic-data"},"next":{"title":"Nav2 for Bipedal Humanoids","permalink":"/AI-And-Robotic-Hackathoon/nvidia-isaac/nav2-bipedal-locomotion"}}');var t=i(4848),s=i(8453);const r={sidebar_position:2,title:"Isaac ROS + Hardware-Accelerated VSLAM"},o="Isaac ROS + Hardware-Accelerated VSLAM",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Theory: Visual-Inertial SLAM Fundamentals",id:"theory-visual-inertial-slam-fundamentals",level:2},{value:"Visual-Inertial Odometry (VIO)",id:"visual-inertial-odometry-vio",level:3},{value:"SLAM Backend Optimization",id:"slam-backend-optimization",level:3},{value:"GPU Acceleration Benefits",id:"gpu-acceleration-benefits",level:3},{value:"Practice: Isaac ROS VSLAM Implementation",id:"practice-isaac-ros-vslam-implementation",level:2},{value:"Isaac ROS GEMs Configuration",id:"isaac-ros-gems-configuration",level:3},{value:"Isaac ROS VSLAM Node Implementation",id:"isaac-ros-vslam-node-implementation",level:3},{value:"Isaac ROS Launch File",id:"isaac-ros-launch-file",level:3},{value:"Isaac ROS GEMs Integration Example",id:"isaac-ros-gems-integration-example",level:3},{value:"Active Learning Exercise",id:"active-learning-exercise",level:2},{value:"Worked Example: Black-box to Glass-box - Isaac ROS VSLAM Pipeline",id:"worked-example-black-box-to-glass-box---isaac-ros-vslam-pipeline",level:2},{value:"Black-box View",id:"black-box-view",level:3},{value:"Glass-box Implementation",id:"glass-box-implementation",level:3},{value:"Understanding the Implementation",id:"understanding-the-implementation",level:3},{value:"Tiered Assessments",id:"tiered-assessments",level:2},{value:"Tier 1: Basic Understanding",id:"tier-1-basic-understanding",level:3},{value:"Tier 2: Application",id:"tier-2-application",level:3},{value:"Tier 3: Analysis and Synthesis",id:"tier-3-analysis-and-synthesis",level:3},{value:"Mermaid Diagram",id:"mermaid-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"isaac-ros--hardware-accelerated-vslam",children:"Isaac ROS + Hardware-Accelerated VSLAM"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Configure and deploy Isaac ROS GEMs for hardware-accelerated perception"}),"\n",(0,t.jsx)(n.li,{children:"Implement visual-inertial SLAM systems optimized for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Integrate stereo cameras and IMU sensors for robust VSLAM"}),"\n",(0,t.jsx)(n.li,{children:"Optimize perception pipelines for real-time performance on NVIDIA hardware"}),"\n",(0,t.jsx)(n.li,{children:"Evaluate VSLAM system accuracy and computational efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Handle sensor fusion and calibration for multi-modal perception"}),"\n",(0,t.jsx)(n.li,{children:"Deploy VSLAM systems on humanoid robot platforms with GPU acceleration"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is a critical capability for humanoid robots operating in unknown environments. NVIDIA Isaac ROS provides a comprehensive set of GPU-accelerated perception libraries called GEMs (GPU-accelerated Embedded Modules) that enable real-time visual-inertial SLAM on robotic platforms. These GEMs leverage NVIDIA's CUDA cores and Tensor cores to deliver performance that is orders of magnitude faster than CPU-only implementations, making them ideal for resource-constrained humanoid robots that require real-time perception capabilities."}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS GEMs are designed specifically for embedded robotics applications, providing optimized implementations of computer vision and perception algorithms that can run efficiently on NVIDIA Jetson and RTX hardware. For humanoid robots, which typically operate in dynamic, human-populated environments, the ability to perform real-time SLAM is essential for navigation, obstacle avoidance, and interaction with the environment."}),"\n",(0,t.jsx)(n.p,{children:"The integration of visual and inertial sensing creates a robust perception system that can operate in challenging conditions where visual-only SLAM might fail. The IMU provides high-frequency motion information that helps maintain tracking during fast movements or when visual features are sparse, while the camera provides rich geometric and semantic information for mapping and localization."}),"\n",(0,t.jsx)(n.h2,{id:"theory-visual-inertial-slam-fundamentals",children:"Theory: Visual-Inertial SLAM Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"visual-inertial-odometry-vio",children:"Visual-Inertial Odometry (VIO)"}),"\n",(0,t.jsx)(n.p,{children:"Visual-Inertial Odometry combines visual and inertial measurements to estimate the robot's motion. The key insight is that the IMU provides high-frequency (typically 100-400 Hz) measurements of acceleration and angular velocity, while the camera provides lower-frequency but geometrically rich information. The fusion of these sensors provides:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"High-frequency pose estimation"}),": The IMU enables pose prediction between camera frames"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness to motion blur"}),": Fast movements that cause visual blur can be compensated using IMU data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scale observability"}),": Stereo or motion-based depth estimation provides absolute scale"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Drift reduction"}),": Visual relocalization helps correct IMU drift over time"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"slam-backend-optimization",children:"SLAM Backend Optimization"}),"\n",(0,t.jsx)(n.p,{children:"The SLAM backend maintains a map of the environment and optimizes the robot's trajectory and landmark positions. Key components include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Frontend"}),": Feature detection, tracking, and initial pose estimation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Backend"}),": Bundle adjustment, loop closure, and graph optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Mapping"}),": Maintaining a consistent representation of the environment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Loop Closure"}),": Detecting revisited locations to correct accumulated drift"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"gpu-acceleration-benefits",children:"GPU Acceleration Benefits"}),"\n",(0,t.jsx)(n.p,{children:"GPU acceleration provides several benefits for VSLAM systems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parallel Processing"}),": GPUs can process multiple features simultaneously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Matrix Operations"}),": SLAM involves many matrix operations that are highly parallelizable"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning Integration"}),": GPUs enable real-time neural network inference for semantic understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory Bandwidth"}),": High memory bandwidth enables efficient processing of large image datasets"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"practice-isaac-ros-vslam-implementation",children:"Practice: Isaac ROS VSLAM Implementation"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-gems-configuration",children:"Isaac ROS GEMs Configuration"}),"\n",(0,t.jsx)(n.p,{children:"Let's create a comprehensive Isaac ROS VSLAM configuration. First, we'll set up the necessary configuration files:"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"~/ros2_ws/src/isaac_ros_vslam/config/vslam_pipeline_config.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'# Isaac ROS VSLAM Pipeline Configuration\n\n# Camera configuration\ncamera:\n  camera_name: "stereo_camera"\n  image_width: 1920\n  image_height: 1080\n  fps: 30\n  rectified_images: true\n\n  # Left camera parameters\n  left:\n    camera_matrix: [900.0, 0.0, 960.0, 0.0, 900.0, 540.0, 0.0, 0.0, 1.0]\n    distortion_coefficients: [0.0, 0.0, 0.0, 0.0, 0.0]\n    projection_matrix: [900.0, 0.0, 960.0, 0.0, 0.0, 900.0, 540.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n\n  # Right camera parameters\n  right:\n    camera_matrix: [900.0, 0.0, 960.0, 0.0, 900.0, 540.0, 0.0, 0.0, 1.0]\n    distortion_coefficients: [0.0, 0.0, 0.0, 0.0, 0.0]\n    projection_matrix: [900.0, 0.0, 960.0, -45.0, 0.0, 900.0, 540.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n    rectification_matrix: [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n\n# IMU configuration\nimu:\n  topic_name: "/imu/data"\n  rate: 200  # Hz\n  accelerometer_noise_density: 0.017  # m/s^2 / sqrt(Hz)\n  gyroscope_noise_density: 0.0012    # rad/s / sqrt(Hz)\n  accelerometer_random_walk: 0.0006  # m/s^3 / sqrt(Hz)\n  gyroscope_random_walk: 4.8e-06     # rad/s^2 / sqrt(Hz)\n\n# VSLAM parameters\nvslam:\n  # Feature detection\n  feature_detector:\n    max_features: 2000\n    min_distance: 15.0\n    quality_level: 0.01\n    block_size: 15\n    use_harris: false\n    k: 0.04\n\n  # Feature tracking\n  feature_tracker:\n    window_size: [21, 21]\n    max_level: 3\n    max_iterations: 30\n    epsilon: 0.01\n    use_initial_estimate: true\n\n  # Stereo matching\n  stereo_matcher:\n    min_disparity: 0\n    num_disparities: 128\n    block_size: 15\n    uniqueness_ratio: 15\n    speckle_window_size: 200\n    speckle_range: 32\n    disp12_max_diff: 1\n\n  # IMU integration\n  imu_integration:\n    enable: true\n    max_imu_queue_size: 100\n    gravity_threshold: 0.9\n    use_imu_for_initialization: true\n\n  # Optimization\n  optimization:\n    enable_bundle_adjustment: true\n    max_iterations: 100\n    convergence_criteria: 1e-6\n    use_robust_loss_function: true\n    loss_function_parameter: 1.0\n\n# Hardware acceleration settings\nhardware_acceleration:\n  cuda_device_id: 0\n  tensor_cores_enabled: true\n  memory_pool_size: "2GB"\n  enable_mixed_precision: true\n\n# Performance monitoring\nperformance:\n  enable_profiling: true\n  publish_timing_info: true\n  max_processing_time_ms: 33  # For 30 FPS\n  target_processing_time_ms: 20\n'})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-vslam-node-implementation",children:"Isaac ROS VSLAM Node Implementation"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"~/ros2_ws/src/isaac_ros_vslam/src/vslam_node.cpp"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <sensor_msgs/msg/image.hpp>\n#include <sensor_msgs/msg/imu.hpp>\n#include <sensor_msgs/msg/camera_info.hpp>\n#include <geometry_msgs/msg/pose_stamped.hpp>\n#include <nav_msgs/msg/odometry.hpp>\n#include <cv_bridge/cv_bridge.h>\n#include <opencv2/opencv.hpp>\n#include <opencv2/features2d.hpp>\n#include <cuda_runtime.h>\n#include <npp.h>\n\nclass IsaacVSLAMNode : public rclcpp::Node\n{\npublic:\n    IsaacVSLAMNode() : Node("isaac_vslam_node")\n    {\n        // Declare parameters\n        this->declare_parameter<std::string>("camera_namespace", "/stereo_camera");\n        this->declare_parameter<std::string>("imu_topic", "/imu/data");\n        this->declare_parameter<double>("processing_rate", 30.0);\n        this->declare_parameter<bool>("enable_gpu_acceleration", true);\n        this->declare_parameter<int>("cuda_device_id", 0);\n\n        // Get parameters\n        camera_namespace_ = this->get_parameter("camera_namespace").as_string();\n        imu_topic_ = this->get_parameter("imu_topic").as_string();\n        processing_rate_ = this->get_parameter("processing_rate").as_double();\n        enable_gpu_accel_ = this->get_parameter("enable_gpu_acceleration").as_bool();\n        cuda_device_id_ = this->get_parameter("cuda_device_id").as_integer();\n\n        // Initialize CUDA device\n        if (enable_gpu_accel_) {\n            cudaError_t cuda_status = cudaSetDevice(cuda_device_id_);\n            if (cuda_status != cudaSuccess) {\n                RCLCPP_ERROR(this->get_logger(), "Failed to set CUDA device: %s",\n                           cudaGetErrorString(cuda_status));\n                enable_gpu_accel_ = false;\n            } else {\n                RCLCPP_INFO(this->get_logger(), "CUDA device %d initialized", cuda_device_id_);\n            }\n        }\n\n        // Create subscribers\n        left_image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            camera_namespace_ + "/left/image_rect_color",\n            10,\n            std::bind(&IsaacVSLAMNode::leftImageCallback, this, std::placeholders::_1));\n\n        right_image_sub_ = this->create_subscription<sensor_msgs::msg::Image>(\n            camera_namespace_ + "/right/image_rect_color",\n            10,\n            std::bind(&IsaacVSLAMNode::rightImageCallback, this, std::placeholders::_1));\n\n        imu_sub_ = this->create_subscription<sensor_msgs::msg::Imu>(\n            imu_topic_,\n            100,\n            std::bind(&IsaacVSLAMNode::imuCallback, this, std::placeholders::_1));\n\n        left_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            camera_namespace_ + "/left/camera_info",\n            10,\n            std::bind(&IsaacVSLAMNode::leftInfoCallback, this, std::placeholders::_1));\n\n        right_info_sub_ = this->create_subscription<sensor_msgs::msg::CameraInfo>(\n            camera_namespace_ + "/right/camera_info",\n            10,\n            std::bind(&IsaacVSLAMNode::rightInfoCallback, this, std::placeholders::_1));\n\n        // Create publishers\n        pose_pub_ = this->create_publisher<geometry_msgs::msg::PoseStamped>(\n            "vslam/pose", 10);\n\n        odom_pub_ = this->create_publisher<nav_msgs::msg::Odometry>(\n            "vslam/odometry", 10);\n\n        // Initialize feature detector\n        feature_detector_ = cv::ORB::create(2000);  // 2000 features max\n\n        // Initialize stereo matcher (CPU version for compatibility)\n        stereo_matcher_ = cv::StereoBM::create(128, 15);\n\n        RCLCPP_INFO(this->get_logger(), "Isaac VSLAM Node initialized");\n    }\n\nprivate:\n    void leftImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Store left image\n        left_image_ = cv_ptr->image.clone();\n        left_image_timestamp_ = msg->header.stamp;\n\n        // Process stereo pair if both images are available\n        if (!right_image_.empty() &&\n            abs(rclcpp::Time(left_image_timestamp_).seconds() -\n                rclcpp::Time(right_image_timestamp_).seconds()) < 0.033) { // 30fps threshold\n            processStereoPair();\n        }\n    }\n\n    void rightImageCallback(const sensor_msgs::msg::Image::SharedPtr msg)\n    {\n        // Convert ROS image to OpenCV\n        cv_bridge::CvImagePtr cv_ptr;\n        try {\n            cv_ptr = cv_bridge::toCvCopy(msg, sensor_msgs::image_encodings::BGR8);\n        } catch (cv_bridge::Exception& e) {\n            RCLCPP_ERROR(this->get_logger(), "cv_bridge exception: %s", e.what());\n            return;\n        }\n\n        // Store right image\n        right_image_ = cv_ptr->image.clone();\n        right_image_timestamp_ = msg->header.stamp;\n\n        // Process stereo pair if both images are available\n        if (!left_image_.empty() &&\n            abs(rclcpp::Time(left_image_timestamp_).seconds() -\n                rclcpp::Time(right_image_timestamp_).seconds()) < 0.033) { // 30fps threshold\n            processStereoPair();\n        }\n    }\n\n    void imuCallback(const sensor_msgs::msg::Imu::SharedPtr msg)\n    {\n        // Store IMU data for fusion\n        imu_buffer_.push_back(*msg);\n\n        // Keep only recent IMU data (last 1 second)\n        while (imu_buffer_.size() > 200) { // Assuming 200Hz IMU\n            imu_buffer_.pop_front();\n        }\n    }\n\n    void leftInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        left_camera_info_ = *msg;\n        camera_params_updated_ = true;\n    }\n\n    void rightInfoCallback(const sensor_msgs::msg::CameraInfo::SharedPtr msg)\n    {\n        right_camera_info_ = *msg;\n        camera_params_updated_ = true;\n    }\n\n    void processStereoPair()\n    {\n        if (left_image_.empty() || right_image_.empty() || !camera_params_updated_) {\n            return;\n        }\n\n        // Detect features in left image\n        std::vector<cv::KeyPoint> keypoints_left, keypoints_right;\n        cv::Mat descriptors_left, descriptors_right;\n\n        feature_detector_->detectAndCompute(left_image_, cv::noArray(), keypoints_left, descriptors_left);\n\n        // Track features to right image using optical flow (simplified)\n        std::vector<cv::Point2f> points_left, points_right;\n        for (const auto& kp : keypoints_left) {\n            points_left.push_back(kp.pt);\n        }\n\n        if (!points_left.empty()) {\n            std::vector<uchar> status;\n            std::vector<float> error;\n\n            cv::calcOpticalFlowPyrLK(left_image_, right_image_,\n                                   points_left, points_right, status, error);\n\n            // Filter out bad matches\n            std::vector<cv::Point2f> good_points_left, good_points_right;\n            for (size_t i = 0; i < status.size(); ++i) {\n                if (status[i]) {\n                    good_points_left.push_back(points_left[i]);\n                    good_points_right.push_back(points_right[i]);\n                }\n            }\n\n            if (good_points_left.size() >= 8) { // Need at least 8 points for pose estimation\n                // Estimate stereo disparity and depth\n                cv::Mat disparity = estimateDisparity(good_points_left, good_points_right);\n\n                // Perform pose estimation using PnP with depth\n                cv::Mat rvec, tvec;\n                if (estimatePose(good_points_left, disparity, rvec, tvec)) {\n                    // Publish pose and odometry\n                    publishPose(rvec, tvec);\n                    publishOdometry(rvec, tvec);\n                }\n            }\n        }\n\n        // Clear images after processing to save memory\n        left_image_ = cv::Mat();\n        right_image_ = cv::Mat();\n    }\n\n    cv::Mat estimateDisparity(const std::vector<cv::Point2f>& points_left,\n                              const std::vector<cv::Point2f>& points_right)\n    {\n        cv::Mat disparity;\n\n        if (enable_gpu_accel_) {\n            // Use GPU-accelerated stereo matching (simulated)\n            // In real Isaac ROS, this would use Isaac ROS Stereo DNN\n            stereo_matcher_->compute(left_image_, right_image_, disparity);\n        } else {\n            stereo_matcher_->compute(left_image_, right_image_, disparity);\n        }\n\n        return disparity;\n    }\n\n    bool estimatePose(const std::vector<cv::Point2f>& points_left,\n                      const cv::Mat& disparity,\n                      cv::Mat& rvec, cv::Mat& tvec)\n    {\n        // Create 3D points from 2D points + disparity\n        std::vector<cv::Point3f> points_3d;\n\n        // Use camera parameters to triangulate 3D points\n        double fx = left_camera_info_.p[0]; // Focal length x\n        double fy = left_camera_info_.p[5]; // Focal length y\n        double cx = left_camera_info_.p[2]; // Principal point x\n        double cy = left_camera_info_.p[6]; // Principal point y\n        double baseline = -left_camera_info_.p[3] / fx; // Baseline from P matrix\n\n        for (size_t i = 0; i < points_left.size(); ++i) {\n            double u = points_left[i].x;\n            double v = points_left[i].y;\n            double d = disparity.at<short>(v, u) / 16.0; // Convert to float disparity\n\n            if (d > 0) { // Valid disparity\n                double z = fx * baseline / d; // Depth\n                double x = (u - cx) * z / fx;\n                double y = (v - cy) * z / fy;\n\n                points_3d.push_back(cv::Point3f(x, y, z));\n            }\n        }\n\n        if (points_3d.size() < 4) {\n            return false;\n        }\n\n        // Use PnP to estimate pose\n        std::vector<cv::Point2f> image_points = points_left; // Use same points\n\n        // Create dummy 3D points for initialization (in real implementation, use map points)\n        std::vector<cv::Point3f> world_points;\n        for (size_t i = 0; i < std::min(points_3d.size(), image_points.size()); ++i) {\n            world_points.push_back(points_3d[i]);\n        }\n\n        // Solve PnP\n        cv::Mat camera_matrix = (cv::Mat_<double>(3, 3) <<\n                                fx, 0, cx,\n                                0, fy, cy,\n                                0, 0, 1);\n        cv::Mat dist_coeffs = cv::Mat::zeros(4, 1, CV_64F); // Assuming no distortion\n\n        bool success = cv::solvePnP(world_points, image_points, camera_matrix, dist_coeffs, rvec, tvec);\n\n        return success;\n    }\n\n    void publishPose(const cv::Mat& rvec, const cv::Mat& tvec)\n    {\n        auto pose_msg = geometry_msgs::msg::PoseStamped();\n        pose_msg.header.stamp = this->get_clock()->now();\n        pose_msg.header.frame_id = "map";\n\n        // Convert rotation vector to quaternion\n        cv::Mat rotation_matrix;\n        cv::Rodrigues(rvec, rotation_matrix);\n\n        double* rm = rotation_matrix.ptr<double>();\n        double qw = sqrt(1 + rm[0] + rm[4] + rm[8]) / 2.0;\n        double qx = (rm[7] - rm[5]) / (4 * qw);\n        double qy = (rm[2] - rm[6]) / (4 * qw);\n        double qz = (rm[3] - rm[1]) / (4 * qw);\n\n        pose_msg.pose.orientation.w = qw;\n        pose_msg.pose.orientation.x = qx;\n        pose_msg.pose.orientation.y = qy;\n        pose_msg.pose.orientation.z = qz;\n\n        pose_msg.pose.position.x = tvec.at<double>(0);\n        pose_msg.pose.position.y = tvec.at<double>(1);\n        pose_msg.pose.position.z = tvec.at<double>(2);\n\n        pose_pub_->publish(pose_msg);\n    }\n\n    void publishOdometry(const cv::Mat& rvec, const cv::Mat& tvec)\n    {\n        auto odom_msg = nav_msgs::msg::Odometry();\n        odom_msg.header.stamp = this->get_clock()->now();\n        odom_msg.header.frame_id = "map";\n        odom_msg.child_frame_id = "base_link";\n\n        // Position from translation vector\n        odom_msg.pose.pose.position.x = tvec.at<double>(0);\n        odom_msg.pose.pose.position.y = tvec.at<double>(1);\n        odom_msg.pose.pose.position.z = tvec.at<double>(2);\n\n        // Orientation from rotation vector\n        cv::Mat rotation_matrix;\n        cv::Rodrigues(rvec, rotation_matrix);\n\n        double* rm = rotation_matrix.ptr<double>();\n        double qw = sqrt(1 + rm[0] + rm[4] + rm[8]) / 2.0;\n        double qx = (rm[7] - rm[5]) / (4 * qw);\n        double qy = (rm[2] - rm[6]) / (4 * qw);\n        double qz = (rm[3] - rm[1]) / (4 * qw);\n\n        odom_msg.pose.pose.orientation.w = qw;\n        odom_msg.pose.pose.orientation.x = qx;\n        odom_msg.pose.pose.orientation.y = qy;\n        odom_msg.pose.pose.orientation.z = qz;\n\n        // Set zero velocity for now (would come from IMU integration in real system)\n        odom_msg.twist.twist.linear.x = 0.0;\n        odom_msg.twist.twist.linear.y = 0.0;\n        odom_msg.twist.twist.linear.z = 0.0;\n        odom_msg.twist.twist.angular.x = 0.0;\n        odom_msg.twist.twist.angular.y = 0.0;\n        odom_msg.twist.twist.angular.z = 0.0;\n\n        odom_pub_->publish(odom_msg);\n    }\n\n    // ROS components\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr left_image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Image>::SharedPtr right_image_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::Imu>::SharedPtr imu_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr left_info_sub_;\n    rclcpp::Subscription<sensor_msgs::msg::CameraInfo>::SharedPtr right_info_sub_;\n    rclcpp::Publisher<geometry_msgs::msg::PoseStamped>::SharedPtr pose_pub_;\n    rclcpp::Publisher<nav_msgs::msg::Odometry>::SharedPtr odom_pub_;\n\n    // Camera and sensor data\n    cv::Mat left_image_, right_image_;\n    builtin_interfaces::msg::Time left_image_timestamp_, right_image_timestamp_;\n    sensor_msgs::msg::CameraInfo left_camera_info_, right_camera_info_;\n    bool camera_params_updated_ = false;\n\n    // IMU data buffer\n    std::deque<sensor_msgs::msg::Imu> imu_buffer_;\n\n    // Feature detection and matching\n    cv::Ptr<cv::Feature2D> feature_detector_;\n    cv::Ptr<cv::StereoBM> stereo_matcher_;\n\n    // Configuration\n    std::string camera_namespace_;\n    std::string imu_topic_;\n    double processing_rate_;\n    bool enable_gpu_accel_;\n    int cuda_device_id_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<IsaacVSLAMNode>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-launch-file",children:"Isaac ROS Launch File"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"~/ros2_ws/src/isaac_ros_vslam/launch/vslam_pipeline.launch.py"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import os\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n    # Declare launch arguments\n    camera_namespace_arg = DeclareLaunchArgument(\n        'camera_namespace',\n        default_value='/stereo_camera',\n        description='Namespace for stereo camera topics'\n    )\n\n    imu_topic_arg = DeclareLaunchArgument(\n        'imu_topic',\n        default_value='/imu/data',\n        description='Topic name for IMU data'\n    )\n\n    processing_rate_arg = DeclareLaunchArgument(\n        'processing_rate',\n        default_value='30.0',\n        description='Processing rate for VSLAM pipeline'\n    )\n\n    cuda_device_arg = DeclareLaunchArgument(\n        'cuda_device',\n        default_value='0',\n        description='CUDA device ID for GPU acceleration'\n    )\n\n    # Get launch configurations\n    camera_namespace = LaunchConfiguration('camera_namespace')\n    imu_topic = LaunchConfiguration('imu_topic')\n    processing_rate = LaunchConfiguration('processing_rate')\n    cuda_device = LaunchConfiguration('cuda_device')\n\n    # Isaac ROS VSLAM node\n    vslam_node = Node(\n        package='isaac_ros_vslam',\n        executable='vslam_node',\n        name='isaac_vslam',\n        parameters=[\n            {\n                'camera_namespace': camera_namespace,\n                'imu_topic': imu_topic,\n                'processing_rate': processing_rate,\n                'enable_gpu_acceleration': True,\n                'cuda_device_id': cuda_device\n            }\n        ],\n        remappings=[\n            ('vslam/pose', 'visual_odometry/pose'),\n            ('vslam/odometry', 'visual_odometry/odometry')\n        ],\n        output='screen'\n    )\n\n    # Isaac ROS stereo image rectification (if needed)\n    stereo_rectify_node = Node(\n        package='isaac_ros_stereo_image_proc',\n        executable='stereo_rectify_node',\n        name='stereo_rectify',\n        parameters=[\n            {\n                'camera_namespace': camera_namespace,\n                'queue_size': 10,\n                'use_camera_info': True\n            }\n        ],\n        output='screen'\n    )\n\n    # Isaac ROS feature detection node (GPU accelerated)\n    feature_detection_node = Node(\n        package='isaac_ros_feature_detection',\n        executable='feature_detection_node',\n        name='feature_detection',\n        parameters=[\n            {\n                'max_features': 2000,\n                'min_distance': 15.0,\n                'quality_level': 0.01,\n                'enable_gpu_acceleration': True\n            }\n        ],\n        output='screen'\n    )\n\n    # Isaac ROS stereo matching node (GPU accelerated)\n    stereo_match_node = Node(\n        package='isaac_ros_stereo_disparity',\n        executable='stereo_disparity_node',\n        name='stereo_matching',\n        parameters=[\n            {\n                'min_disparity': 0,\n                'num_disparities': 128,\n                'block_size': 15,\n                'enable_gpu_acceleration': True\n            }\n        ],\n        output='screen'\n    )\n\n    return LaunchDescription([\n        camera_namespace_arg,\n        imu_topic_arg,\n        processing_rate_arg,\n        cuda_device_arg,\n        stereo_rectify_node,\n        feature_detection_node,\n        stereo_match_node,\n        vslam_node\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-gems-integration-example",children:"Isaac ROS GEMs Integration Example"}),"\n",(0,t.jsxs)(n.p,{children:["Create ",(0,t.jsx)(n.code,{children:"~/ros2_ws/src/isaac_ros_vslam/src/gem_integration_example.cpp"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:'#include <rclcpp/rclcpp.hpp>\n#include <isaac_ros_visual_slam/visual_slam_node.hpp>\n#include <isaac_ros_stereo_image_proc/stereo_rectification_node.hpp>\n#include <isaac_ros_feature_detection/feature_detection_node.hpp>\n#include <isaac_ros_stereo_disparity/stereo_disparity_node.hpp>\n\nclass IsaacVSLAMGEMsIntegration : public rclcpp::Node\n{\npublic:\n    IsaacVSLAMGEMsIntegration() : Node("isaac_vslam_gems_integration")\n    {\n        // This would integrate actual Isaac ROS GEMs\n        // For demonstration, we\'ll outline the structure\n\n        RCLCPP_INFO(this->get_logger(), "Isaac ROS GEMs Integration Node initialized");\n\n        // In a real implementation, you would:\n        // 1. Initialize the Visual SLAM GEM\n        // 2. Configure stereo rectification\n        // 3. Set up feature detection with GPU acceleration\n        // 4. Configure stereo disparity estimation\n        // 5. Integrate IMU data for VIO\n    }\n\nprivate:\n    // In a real implementation, these would be actual GEM instances\n    // std::shared_ptr<VisualSlamNode> visual_slam_node_;\n    // std::shared_ptr<StereoRectificationNode> stereo_rect_node_;\n    // std::shared_ptr<FeatureDetectionNode> feature_det_node_;\n    // std::shared_ptr<StereoDisparityNode> stereo_disp_node_;\n};\n\nint main(int argc, char * argv[])\n{\n    rclcpp::init(argc, argv);\n    rclcpp::spin(std::make_shared<IsaacVSLAMGEMsIntegration>());\n    rclcpp::shutdown();\n    return 0;\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"active-learning-exercise",children:"Active Learning Exercise"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Exercise: VSLAM Performance Optimization"})}),"\n",(0,t.jsx)(n.p,{children:"Implement and compare different optimization strategies for your VSLAM system:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Feature Management"}),": Compare different feature detection algorithms (ORB, FAST, Shi-Tomasi) and evaluate their performance vs. accuracy trade-offs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU Memory Management"}),": Implement memory pooling and reuse strategies to optimize GPU memory usage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-threading"}),": Design a multi-threaded architecture that processes different components in parallel"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Adaptive Processing"}),": Create a system that adjusts processing parameters based on computational load"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Test your implementations with different scene complexities and motion patterns, then analyze the performance vs. accuracy trade-offs."}),"\n",(0,t.jsx)(n.h2,{id:"worked-example-black-box-to-glass-box---isaac-ros-vslam-pipeline",children:"Worked Example: Black-box to Glass-box - Isaac ROS VSLAM Pipeline"}),"\n",(0,t.jsx)(n.h3,{id:"black-box-view",children:"Black-box View"}),"\n",(0,t.jsx)(n.p,{children:"We'll create a complete Isaac ROS VSLAM pipeline that takes stereo camera images and IMU data as input and outputs real-time pose estimates and maps. The black-box view is: the system receives sensor data and produces accurate localization and mapping results in real-time."}),"\n",(0,t.jsx)(n.h3,{id:"glass-box-implementation",children:"Glass-box Implementation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Complete pipeline architecture:"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The implementation includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Stereo image rectification"}),"\n",(0,t.jsx)(n.li,{children:"Feature detection and tracking (GPU accelerated)"}),"\n",(0,t.jsx)(n.li,{children:"Stereo disparity estimation"}),"\n",(0,t.jsx)(n.li,{children:"Visual-inertial fusion"}),"\n",(0,t.jsx)(n.li,{children:"Pose estimation and optimization"}),"\n",(0,t.jsx)(n.li,{children:"Map building and maintenance"}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Hardware acceleration integration:"})}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-cpp",children:"// Example of GPU-accelerated feature detection (conceptual)\nclass GPUFeatureDetector {\npublic:\n    GPUFeatureDetector(int max_features = 2000) : max_features_(max_features) {\n        // Initialize CUDA streams and memory pools\n        cudaStreamCreate(&stream_);\n\n        // Allocate GPU memory for image processing\n        cudaMalloc(&d_image_, IMAGE_WIDTH * IMAGE_HEIGHT * sizeof(uchar));\n        cudaMalloc(&d_features_, max_features_ * sizeof(FeaturePoint));\n    }\n\n    void detectFeatures(const cv::Mat& image, std::vector<cv::KeyPoint>& keypoints) {\n        // Copy image to GPU\n        cudaMemcpyAsync(d_image_, image.data,\n                       image.total() * image.elemSize(),\n                       cudaMemcpyHostToDevice, stream_);\n\n        // Launch GPU kernel for feature detection\n        detect_features_kernel<<<blocks, threads, 0, stream_>>>(\n            d_image_, d_features_, image.cols, image.rows, max_features_);\n\n        // Copy results back to CPU\n        cudaMemcpyAsync(h_features_, d_features_,\n                       max_features_ * sizeof(FeaturePoint),\n                       cudaMemcpyDeviceToHost, stream_);\n\n        cudaStreamSynchronize(stream_);\n\n        // Convert to OpenCV keypoints\n        convertToKeyPoints(h_features_, keypoints);\n    }\n\nprivate:\n    int max_features_;\n    cudaStream_t stream_;\n    uchar* d_image_;\n    FeaturePoint* d_features_;\n    FeaturePoint* h_features_;\n};\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Real-time optimization:"})}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The system implements real-time optimization techniques:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Keyframe selection to reduce computational load"}),"\n",(0,t.jsx)(n.li,{children:"Windowed bundle adjustment for efficiency"}),"\n",(0,t.jsx)(n.li,{children:"Multi-resolution processing for speed"}),"\n",(0,t.jsx)(n.li,{children:"Adaptive feature management based on scene complexity"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"understanding-the-implementation",children:"Understanding the Implementation"}),"\n",(0,t.jsx)(n.p,{children:"The glass-box view reveals:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The pipeline efficiently utilizes GPU acceleration for computationally intensive tasks"}),"\n",(0,t.jsx)(n.li,{children:"Sensor fusion combines visual and inertial data for robust tracking"}),"\n",(0,t.jsx)(n.li,{children:"Real-time performance is maintained through optimized algorithms and memory management"}),"\n",(0,t.jsx)(n.li,{children:"The system is designed to handle the specific requirements of humanoid robot navigation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"tiered-assessments",children:"Tiered Assessments"}),"\n",(0,t.jsx)(n.h3,{id:"tier-1-basic-understanding",children:"Tier 1: Basic Understanding"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"What are Isaac ROS GEMs and what is their purpose?"}),"\n",(0,t.jsx)(n.li,{children:"Name three benefits of GPU acceleration for VSLAM systems."}),"\n",(0,t.jsx)(n.li,{children:"What is the difference between visual odometry and SLAM?"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tier-2-application",children:"Tier 2: Application"}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsx)(n.li,{children:"Configure an Isaac ROS VSLAM pipeline with stereo cameras and IMU integration."}),"\n",(0,t.jsx)(n.li,{children:"Implement a basic feature tracking algorithm with GPU acceleration."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"tier-3-analysis-and-synthesis",children:"Tier 3: Analysis and Synthesis"}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsx)(n.li,{children:"Design a complete VSLAM system for humanoid robot navigation that includes real-time performance optimization, robust sensor fusion, and failure recovery mechanisms."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"mermaid-diagram",children:"Mermaid Diagram"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Stereo Cameras] --\x3e B[Image Rectification]\n    C[IMU Sensor] --\x3e D[Inertial Processing]\n\n    B --\x3e E[Feature Detection<br/>GPU Accelerated]\n    D --\x3e F[Preintegration Factors]\n\n    E --\x3e G[Stereo Matching<br/>GPU Accelerated]\n    F --\x3e H[Visual-Inertial Fusion]\n\n    G --\x3e I[3D Point Cloud]\n    H --\x3e J[Pose Estimation]\n\n    I --\x3e K[Map Building]\n    J --\x3e L[Odometry Output]\n\n    K --\x3e M[Loop Closure]\n    L --\x3e N[Robot Localization]\n\n    M --\x3e O[Graph Optimization]\n    O --\x3e K\n    O --\x3e J\n\n    N --\x3e P[Navigation System]\n    P --\x3e A\n    P --\x3e C\n\n    style A fill:#ff9999\n    style C fill:#ff9999\n    style E fill:#99ccff\n    style G fill:#99ccff\n    style P fill:#99ff99\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Alt-text for diagram:"}),' "Isaac ROS VSLAM pipeline showing stereo cameras and IMU sensor feeding into image rectification and inertial processing respectively. Image rectification leads to GPU-accelerated feature detection, which connects to stereo matching. Inertial processing connects to preintegration factors. Both paths feed into visual-inertial fusion, leading to 3D point cloud and pose estimation. The 3D point cloud feeds into map building, while pose estimation provides odometry output. Map building connects to loop closure, which feeds into graph optimization that improves both mapping and pose estimation. The final robot localization output connects to the navigation system, which closes the loop back to the sensors. Sensors are highlighted in pink, GPU-accelerated components in light blue, and navigation system in light green."']}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"This chapter covered the implementation of hardware-accelerated Visual SLAM systems using NVIDIA Isaac ROS GEMs for humanoid robots. We explored the theoretical foundations of visual-inertial SLAM, implemented practical pipeline configurations, and demonstrated how to leverage GPU acceleration for real-time performance. The examples showed how to integrate stereo cameras and IMU sensors for robust perception in dynamic environments."}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"NVIDIA. (2022). Isaac ROS Visual SLAM: Developer Guide. NVIDIA Corporation."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Mur-Artal, R., & Tard\xf3s, J. D. (2017). ORB-SLAM2: an open-source SLAM system for monocular, stereo, and RGB-D cameras. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 33(5), 1255-1262."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Qin, T., Li, P., & Shen, S. (2018). VINS-mono: A robust and versatile monocular visual-inertial state estimator. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 34(4), 1004-1020."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Usenko, V., Demmel, N., Schubert, D., St\xfcckler, J., & Cremers, D. (2020). Visual-inertial mapping with non-linear factor recovery. ",(0,t.jsx)(n.em,{children:"IEEE Robotics and Automation Letters"}),", 5(4), 6251-6258."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Forster, C., Carlone, L., Dellaert, F., & Scaramuzza, D. (2017). On-manifold preintegration for real-time visual-inertial odometry. ",(0,t.jsx)(n.em,{children:"IEEE Transactions on Robotics"}),", 33(1), 1-21."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Bloesch, M., Omari, S., Hutter, M., & Siegwart, R. (2015). Robust visual inertial odometry using a direct EKF-based approach. ",(0,t.jsx)(n.em,{children:"2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"}),", 298-304."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Leutenegger, S., Chli, M., & Siegwart, R. Y. (2015). Keyframe-based visual-inertial odometry using nonlinear optimization. ",(0,t.jsx)(n.em,{children:"The International Journal of Robotics Research"}),", 34(3), 314-334."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Qin, T., Yang, Y., & Shen, S. (2020). A general optimization-based framework for global pose estimation with multiple sensors. ",(0,t.jsx)(n.em,{children:"arXiv preprint arXiv:2003.00967"}),"."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);