"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[978],{4484:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"vla/multi-modal-integration","title":"Multi-Modal Integration","description":"Learning Objectives","source":"@site/docs/04-vla/03-multi-modal-integration.md","sourceDirName":"04-vla","slug":"/vla/multi-modal-integration","permalink":"/AI-And-Robotic-Hackathoon/vla/multi-modal-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/mfsrajput/AI-And-Robotic-Hackathoon/edit/main/docs/04-vla/03-multi-modal-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Multi-Modal Integration"},"sidebar":"moduleSidebar","previous":{"title":"Chapter 2: LLM Task & Motion Planning (Natural Language \u2192 ROS Actions)","permalink":"/AI-And-Robotic-Hackathoon/vla/llm-task-and-motion-planning"},"next":{"title":"Chapter 4: Capstone: Autonomous Humanoid (Complete End-to-End System)","permalink":"/AI-And-Robotic-Hackathoon/vla/capstone-autonomous-humanoid"}}');var a=t(4848),s=t(8453);const o={sidebar_position:3,title:"Multi-Modal Integration"},r="Multi-Modal Integration",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Theory: Multi-Modal Perception Fundamentals",id:"theory-multi-modal-perception-fundamentals",level:2},{value:"Cross-Modal Attention Mechanisms",id:"cross-modal-attention-mechanisms",level:3},{value:"Multimodal Fusion Strategies",id:"multimodal-fusion-strategies",level:3},{value:"Temporal Integration",id:"temporal-integration",level:3},{value:"Practice: Implementing Multi-Modal Systems",id:"practice-implementing-multi-modal-systems",level:2},{value:"Multi-Modal Perception Node",id:"multi-modal-perception-node",level:3},{value:"Multi-Modal Fusion Node",id:"multi-modal-fusion-node",level:3},{value:"Vision-Language Processor Node",id:"vision-language-processor-node",level:3},{value:"Action Selection Node",id:"action-selection-node",level:3},{value:"Active Learning Exercise",id:"active-learning-exercise",level:2},{value:"Worked Example: Black-box to Glass-box - Multi-Modal Integration System",id:"worked-example-black-box-to-glass-box---multi-modal-integration-system",level:2},{value:"Black-box View",id:"black-box-view",level:3},{value:"Glass-box Implementation",id:"glass-box-implementation",level:3},{value:"Understanding the Implementation",id:"understanding-the-implementation",level:3},{value:"Tiered Assessments",id:"tiered-assessments",level:2},{value:"Tier 1: Basic Understanding",id:"tier-1-basic-understanding",level:3},{value:"Tier 2: Application",id:"tier-2-application",level:3},{value:"Tier 3: Analysis and Synthesis",id:"tier-3-analysis-and-synthesis",level:3},{value:"Mermaid Diagram",id:"mermaid-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"References",id:"references",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"multi-modal-integration",children:"Multi-Modal Integration"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Design and implement multi-modal perception systems combining vision, language, and action"}),"\n",(0,a.jsx)(n.li,{children:"Integrate visual and linguistic information for enhanced robot understanding"}),"\n",(0,a.jsx)(n.li,{children:"Create cross-modal attention mechanisms for unified perception"}),"\n",(0,a.jsx)(n.li,{children:"Implement multimodal fusion techniques for decision making"}),"\n",(0,a.jsx)(n.li,{children:"Handle temporal synchronization across different modalities"}),"\n",(0,a.jsx)(n.li,{children:"Develop robust multimodal systems that handle sensor failures gracefully"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate multimodal system performance and identify failure modes"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"Multi-Modal Integration (MMI) represents the convergence of multiple sensory modalities\u2014vision, language, and action\u2014into a unified cognitive architecture for humanoid robots. Unlike unimodal systems that process individual sensory streams independently, multimodal integration enables robots to form coherent understanding by combining visual, linguistic, and motor information. This integration is essential for humanoid robots operating in human-centric environments where they must interpret complex scenes described in natural language and execute corresponding actions."}),"\n",(0,a.jsx)(n.p,{children:'The human brain naturally integrates information from multiple senses to form a unified understanding of the world. Similarly, effective humanoid robots must integrate visual perception (what they see), linguistic understanding (what they hear), and motor planning (what they do) to achieve human-like interaction capabilities. This integration allows robots to perform complex tasks such as "Bring me the red cup on the left side of the table," which requires understanding spatial relationships, object recognition, and navigation planning.'}),"\n",(0,a.jsx)(n.p,{children:"Modern multimodal systems leverage deep learning architectures that can process and fuse information from different modalities. Vision-Language-Action (VLA) models represent a significant advancement in this field, enabling end-to-end learning of multimodal policies that can directly map sensory inputs to robot actions. These systems are particularly valuable for humanoid robots due to their ability to handle the rich, multi-dimensional nature of human environments."}),"\n",(0,a.jsx)(n.h2,{id:"theory-multi-modal-perception-fundamentals",children:"Theory: Multi-Modal Perception Fundamentals"}),"\n",(0,a.jsx)(n.h3,{id:"cross-modal-attention-mechanisms",children:"Cross-Modal Attention Mechanisms"}),"\n",(0,a.jsx)(n.p,{children:"Cross-modal attention allows different sensory modalities to influence each other through attention mechanisms. In vision-language systems, for example, language can guide visual attention to relevant parts of a scene, while visual information can ground linguistic concepts in perceptual reality."}),"\n",(0,a.jsx)(n.p,{children:"The attention mechanism can be formalized as:"}),"\n",(0,a.jsx)(n.p,{children:"Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V"}),"\n",(0,a.jsx)(n.p,{children:"Where Q, K, V represent queries, keys, and values from different modalities, allowing information from one modality to attend to relevant information in another."}),"\n",(0,a.jsx)(n.h3,{id:"multimodal-fusion-strategies",children:"Multimodal Fusion Strategies"}),"\n",(0,a.jsx)(n.p,{children:"Different strategies exist for combining information from multiple modalities:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Early Fusion"}),": Combining raw sensory data before processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Late Fusion"}),": Combining high-level features after individual processing"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Intermediate Fusion"}),": Combining features at multiple processing levels"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Attention-based Fusion"}),": Using attention mechanisms to weight different modalities"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"temporal-integration",children:"Temporal Integration"}),"\n",(0,a.jsx)(n.p,{children:"Multi-modal systems must handle temporal aspects of perception and action:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synchronization"}),": Aligning data from different sensors with different sampling rates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory"}),": Maintaining temporal context across modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Prediction"}),": Using temporal patterns to predict future states across modalities"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"practice-implementing-multi-modal-systems",children:"Practice: Implementing Multi-Modal Systems"}),"\n",(0,a.jsx)(n.h3,{id:"multi-modal-perception-node",children:"Multi-Modal Perception Node"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a comprehensive multi-modal integration system. First, we'll set up the package structure:"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/multi_modal_integration/setup.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"from setuptools import find_packages, setup\n\npackage_name = 'multi_modal_integration'\n\nsetup(\n    name=package_name,\n    version='0.0.0',\n    packages=find_packages(exclude=['test']),\n    data_files=[\n        ('share/ament_index/resource_index/packages',\n            ['resource/' + package_name]),\n        ('share/' + package_name, ['package.xml']),\n    ],\n    install_requires=['setuptools'],\n    zip_safe=True,\n    maintainer='robotics',\n    maintainer_email='robotics@example.com',\n    description='Multi-modal integration for vision-language-action systems',\n    license='Apache-2.0',\n    tests_require=['pytest'],\n    entry_points={\n        'console_scripts': [\n            'multi_modal_fusion = multi_modal_integration.multi_modal_fusion:main',\n            'vision_language_processor = multi_modal_integration.vision_language_processor:main',\n            'action_selector = multi_modal_integration.action_selector:main',\n        ],\n    },\n)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/multi_modal_integration/package.xml"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>\n<package format="3">\n  <name>multi_modal_integration</name>\n  <version>0.0.0</version>\n  <description>Multi-modal integration for vision-language-action systems</description>\n  <maintainer email="robotics@example.com">robotics</maintainer>\n  <license>Apache-2.0</license>\n\n  <depend>rclpy</depend>\n  <depend>std_msgs</depend>\n  <depend>sensor_msgs</depend>\n  <depend>geometry_msgs</depend>\n  <depend>cv_bridge</depend>\n  <depend>message_filters</depend>\n\n  <test_depend>ament_copyright</test_depend>\n  <test_depend>ament_flake8</test_depend>\n  <test_depend>ament_pep257</test_depend>\n  <test_depend>python3-pytest</test_depend>\n\n  <export>\n    <build_type>ament_python</build_type>\n  </export>\n</package>\n'})}),"\n",(0,a.jsx)(n.h3,{id:"multi-modal-fusion-node",children:"Multi-Modal Fusion Node"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/multi_modal_integration/multi_modal_integration/multi_modal_fusion.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport threading\nimport queue\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import deque\n\nclass MultiModalFusion(Node):\n    def __init__(self):\n        super().__init__('multi_modal_fusion')\n\n        # Declare parameters\n        self.declare_parameter('image_queue_size', 10)\n        self.declare_parameter('text_queue_size', 10)\n        self.declare_parameter('fusion_frequency', 10.0)\n        self.declare_parameter('temporal_window', 1.0)  # seconds\n        self.declare_parameter('enable_cross_attention', True)\n\n        # Get parameters\n        self.image_queue_size = self.get_parameter('image_queue_size').value\n        self.text_queue_size = self.get_parameter('text_queue_size').value\n        self.fusion_frequency = self.get_parameter('fusion_frequency').value\n        self.temporal_window = self.get_parameter('temporal_window').value\n        self.enable_cross_attention = self.get_parameter('enable_cross_attention').value\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_raw',\n            self.image_callback,\n            self.image_queue_size\n        )\n\n        self.text_sub = self.create_subscription(\n            String,\n            '/natural_language_input',\n            self.text_callback,\n            self.text_queue_size\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            '/camera/rgb/camera_info',\n            self.camera_info_callback,\n            10\n        )\n\n        # Create publishers\n        self.fused_output_pub = self.create_publisher(\n            String,\n            '/multi_modal_output',\n            10\n        )\n\n        self.attention_map_pub = self.create_publisher(\n            Image,\n            '/attention_map',\n            10\n        )\n\n        self.scene_description_pub = self.create_publisher(\n            String,\n            '/scene_description',\n            10\n        )\n\n        # Data buffers\n        self.image_buffer = deque(maxlen=10)\n        self.text_buffer = deque(maxlen=10)\n        self.camera_info = None\n\n        # Fusion model\n        self.fusion_model = self.initialize_fusion_model()\n\n        # Processing queue\n        self.processing_queue = queue.Queue()\n        self.processing_thread = threading.Thread(target=self.process_fusion, daemon=True)\n        self.processing_thread.start()\n\n        # Create timer for fusion processing\n        self.fusion_timer = self.create_timer(\n            1.0 / self.fusion_frequency,\n            self.fusion_callback\n        )\n\n        self.get_logger().info('Multi-Modal Fusion node initialized')\n\n    def initialize_fusion_model(self):\n        \"\"\"Initialize the multi-modal fusion model\"\"\"\n        # In a real implementation, this would load a pre-trained model\n        # For demonstration, we'll create a simple fusion network\n        class SimpleFusionModel(nn.Module):\n            def __init__(self, visual_dim=512, text_dim=512, fusion_dim=512):\n                super().__init__()\n                self.visual_project = nn.Linear(visual_dim, fusion_dim)\n                self.text_project = nn.Linear(text_dim, fusion_dim)\n                self.fusion_layer = nn.Linear(fusion_dim * 2, fusion_dim)\n                self.output_layer = nn.Linear(fusion_dim, 256)  # Output features\n\n            def forward(self, visual_features, text_features):\n                # Project features to common space\n                vis_proj = torch.relu(self.visual_project(visual_features))\n                text_proj = torch.relu(self.text_project(text_features))\n\n                # Cross-attention mechanism\n                attention_weights = torch.softmax(\n                    torch.matmul(vis_proj, text_proj.transpose(-2, -1)) / np.sqrt(vis_proj.size(-1)),\n                    dim=-1\n                )\n                attended_visual = torch.matmul(attention_weights, vis_proj)\n\n                # Concatenate and fuse\n                fused_features = torch.cat([attended_visual, text_proj], dim=-1)\n                fused = torch.relu(self.fusion_layer(fused_features))\n                output = self.output_layer(fused)\n\n                return output, attention_weights\n\n        return SimpleFusionModel()\n\n    def image_callback(self, msg):\n        \"\"\"Handle incoming image messages\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Process image to extract features (simplified)\n            features = self.extract_visual_features(cv_image)\n\n            timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n            self.image_buffer.append({\n                'image': cv_image,\n                'features': features,\n                'timestamp': timestamp,\n                'msg': msg\n            })\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def text_callback(self, msg):\n        \"\"\"Handle incoming text messages\"\"\"\n        # Process text to extract features (simplified)\n        features = self.extract_text_features(msg.data)\n\n        self.text_buffer.append({\n            'text': msg.data,\n            'features': features,\n            'timestamp': time.time()\n        })\n\n    def camera_info_callback(self, msg):\n        \"\"\"Handle camera info updates\"\"\"\n        self.camera_info = msg\n\n    def extract_visual_features(self, image):\n        \"\"\"Extract visual features from image\"\"\"\n        # In a real implementation, this would use a CNN or transformer\n        # For demonstration, we'll use a simple approach\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Extract simple features (mean, std, etc.)\n        features = np.array([\n            np.mean(gray),\n            np.std(gray),\n            gray.shape[0],  # height\n            gray.shape[1],  # width\n        ], dtype=np.float32)\n\n        # Normalize features\n        features = (features - np.mean(features)) / (np.std(features) + 1e-8)\n\n        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n\n    def extract_text_features(self, text):\n        \"\"\"Extract text features from natural language\"\"\"\n        # In a real implementation, this would use an NLP model\n        # For demonstration, we'll use a simple approach\n        # Count basic features: word count, character count, etc.\n        features = np.array([\n            len(text),  # character count\n            len(text.split()),  # word count\n            text.lower().count('the'),  # common word count\n            text.lower().count('and'),  # common word count\n            1 if 'move' in text.lower() else 0,  # action indicator\n            1 if 'object' in text.lower() else 0,  # object indicator\n        ], dtype=np.float32)\n\n        # Normalize features\n        features = (features - np.mean(features)) / (np.std(features) + 1e-8)\n\n        return torch.tensor(features, dtype=torch.float32).unsqueeze(0)\n\n    def fusion_callback(self):\n        \"\"\"Main fusion callback\"\"\"\n        # Get synchronized data\n        sync_data = self.get_synchronized_data()\n\n        if sync_data:\n            # Add to processing queue\n            self.processing_queue.put(sync_data)\n\n    def get_synchronized_data(self) -> Optional[Dict]:\n        \"\"\"Get synchronized visual and text data\"\"\"\n        if not self.image_buffer or not self.text_buffer:\n            return None\n\n        # Find the most recent text that's within temporal window of an image\n        current_time = time.time()\n\n        # Find valid text within temporal window\n        valid_texts = []\n        for text_data in list(self.text_buffer):\n            if current_time - text_data['timestamp'] <= self.temporal_window:\n                valid_texts.append(text_data)\n\n        if not valid_texts:\n            return None\n\n        # Use the most recent text\n        text_data = valid_texts[-1]\n\n        # Find the most temporally aligned image\n        best_image_data = None\n        min_time_diff = float('inf')\n\n        for image_data in list(self.image_buffer):\n            time_diff = abs(text_data['timestamp'] - image_data['timestamp'])\n            if time_diff < min_time_diff:\n                min_time_diff = time_diff\n                best_image_data = image_data\n\n        if best_image_data and min_time_diff <= self.temporal_window:\n            return {\n                'image_data': best_image_data,\n                'text_data': text_data,\n                'time_diff': min_time_diff\n            }\n\n        return None\n\n    def process_fusion(self):\n        \"\"\"Process fusion in a separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                # Get next fusion task from queue\n                try:\n                    sync_data = self.processing_queue.get(timeout=1.0)\n                    self.perform_fusion(sync_data)\n                    self.processing_queue.task_done()\n                except queue.Empty:\n                    continue\n            except Exception as e:\n                self.get_logger().error(f'Error in fusion processing thread: {e}')\n                time.sleep(0.1)\n\n    def perform_fusion(self, sync_data: Dict):\n        \"\"\"Perform multi-modal fusion\"\"\"\n        image_data = sync_data['image_data']\n        text_data = sync_data['text_data']\n\n        try:\n            # Convert to tensors for model\n            visual_tensor = image_data['features']\n            text_tensor = text_data['features']\n\n            # Perform fusion using the model\n            with torch.no_grad():\n                fused_output, attention_weights = self.fusion_model(visual_tensor, text_tensor)\n\n            # Generate fused output\n            fused_result = {\n                'visual_features': visual_tensor.numpy().tolist(),\n                'text_features': text_tensor.numpy().tolist(),\n                'fused_features': fused_output.numpy().tolist(),\n                'attention_weights': attention_weights.numpy().tolist(),\n                'input_text': text_data['text'],\n                'timestamp': time.time()\n            }\n\n            # Publish fused output\n            output_msg = String()\n            output_msg.data = str(fused_result)\n            self.fused_output_pub.publish(output_msg)\n\n            # Generate and publish attention map visualization\n            attention_map = self.generate_attention_map(\n                image_data['image'], attention_weights.numpy()\n            )\n            if attention_map is not None:\n                attention_msg = self.cv_bridge.cv2_to_imgmsg(attention_map, encoding='bgr8')\n                attention_msg.header.stamp = image_data['msg'].header.stamp\n                attention_msg.header.frame_id = image_data['msg'].header.frame_id\n                self.attention_map_pub.publish(attention_msg)\n\n            # Generate scene description\n            scene_description = self.generate_scene_description(\n                image_data['image'], text_data['text']\n            )\n            desc_msg = String()\n            desc_msg.data = scene_description\n            self.scene_description_pub.publish(desc_msg)\n\n            self.get_logger().info(f'Fusion completed: {text_data[\"text\"]}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in fusion: {e}')\n\n    def generate_attention_map(self, image, attention_weights):\n        \"\"\"Generate attention map visualization\"\"\"\n        if attention_weights.size == 0:\n            return image\n\n        # Create attention heatmap\n        attention_map = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n\n        # For simplicity, we'll create a basic attention visualization\n        # In a real implementation, this would use the actual attention weights\n        attention_map = np.ones((image.shape[0], image.shape[1]), dtype=np.float32) * 0.5\n\n        # Apply attention as overlay\n        attention_colored = cv2.applyColorMap(\n            (attention_map * 255).astype(np.uint8), cv2.COLORMAP_JET\n        )\n\n        # Blend with original image\n        blended = cv2.addWeighted(image, 0.7, attention_colored, 0.3, 0)\n\n        return blended\n\n    def generate_scene_description(self, image, text_command):\n        \"\"\"Generate a scene description based on image and text\"\"\"\n        # In a real implementation, this would use a vision-language model\n        # For demonstration, we'll create a simple description\n\n        height, width = image.shape[:2]\n\n        description = f\"\"\"\n        Scene Analysis:\n        - Image dimensions: {width}x{height}\n        - Text command: \"{text_command}\"\n        - Objects detected: [simulated detection results]\n        - Spatial relationships: [simulated spatial analysis]\n        - Action plan: [simulated action plan based on command]\n        \"\"\"\n\n        return description\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    fusion_node = MultiModalFusion()\n\n    try:\n        rclpy.spin(fusion_node)\n    except KeyboardInterrupt:\n        fusion_node.get_logger().info('Shutting down Multi-Modal Fusion')\n    finally:\n        fusion_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"vision-language-processor-node",children:"Vision-Language Processor Node"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/multi_modal_integration/multi_modal_integration/vision_language_processor.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport threading\nimport queue\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import deque\n\nclass VisionLanguageProcessor(Node):\n    def __init__(self):\n        super().__init__('vision_language_processor')\n\n        # Declare parameters\n        self.declare_parameter('image_queue_size', 10)\n        self.declare_parameter('text_queue_size', 10)\n        self.declare_parameter('processing_frequency', 5.0)\n        self.declare_parameter('object_detection_threshold', 0.5)\n        self.declare_parameter('enable_segmentation', True)\n        self.declare_parameter('max_objects', 10)\n\n        # Get parameters\n        self.image_queue_size = self.get_parameter('image_queue_size').value\n        self.text_queue_size = self.get_parameter('text_queue_size').value\n        self.processing_frequency = self.get_parameter('processing_frequency').value\n        self.detection_threshold = self.get_parameter('object_detection_threshold').value\n        self.enable_segmentation = self.get_parameter('enable_segmentation').value\n        self.max_objects = self.get_parameter('max_objects').value\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Create subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_raw',\n            self.image_callback,\n            self.image_queue_size\n        )\n\n        self.text_sub = self.create_subscription(\n            String,\n            '/natural_language_input',\n            self.text_callback,\n            self.text_queue_size\n        )\n\n        # Create publishers\n        self.object_detection_pub = self.create_publisher(\n            String,\n            '/object_detections',\n            10\n        )\n\n        self.spatial_relationships_pub = self.create_publisher(\n            String,\n            '/spatial_relationships',\n            10\n        )\n\n        self.scene_graph_pub = self.create_publisher(\n            String,\n            '/scene_graph',\n            10\n        )\n\n        self.processed_image_pub = self.create_publisher(\n            Image,\n            '/processed_vision_output',\n            10\n        )\n\n        # Data buffers\n        self.image_buffer = deque(maxlen=5)\n        self.text_buffer = deque(maxlen=5)\n\n        # Initialize vision processing models\n        self.vision_model = self.initialize_vision_model()\n        self.text_model = self.initialize_text_model()\n\n        # Processing queue\n        self.processing_queue = queue.Queue()\n        self.processing_thread = threading.Thread(\n            target=self.process_vision_language, daemon=True\n        )\n        self.processing_thread.start()\n\n        # Create timer for processing\n        self.processing_timer = self.create_timer(\n            1.0 / self.processing_frequency,\n            self.processing_callback\n        )\n\n        self.get_logger().info('Vision-Language Processor initialized')\n\n    def initialize_vision_model(self):\n        \"\"\"Initialize vision processing model\"\"\"\n        # In a real implementation, this would load a pre-trained model\n        # For demonstration, we'll create a simple CNN-based detector\n        class SimpleVisionModel(nn.Module):\n            def __init__(self, num_classes=80):\n                super().__init__()\n                # Simple CNN for demonstration\n                self.features = nn.Sequential(\n                    nn.Conv2d(3, 32, 3, padding=1),\n                    nn.ReLU(),\n                    nn.MaxPool2d(2),\n                    nn.Conv2d(32, 64, 3, padding=1),\n                    nn.ReLU(),\n                    nn.MaxPool2d(2),\n                )\n                self.classifier = nn.Linear(64, num_classes)\n                self.bbox_regressor = nn.Linear(64, 4)  # x, y, w, h\n\n            def forward(self, x):\n                features = self.features(x)\n                # Global average pooling\n                features = torch.mean(features, dim=[2, 3])\n\n                class_scores = torch.softmax(self.classifier(features), dim=1)\n                bbox_deltas = self.bbox_regressor(features)\n\n                return class_scores, bbox_deltas\n\n        return SimpleVisionModel()\n\n    def initialize_text_model(self):\n        \"\"\"Initialize text processing model\"\"\"\n        # Simple text model for demonstration\n        class SimpleTextModel(nn.Module):\n            def __init__(self, vocab_size=10000, embedding_dim=128):\n                super().__init__()\n                self.embedding = nn.Embedding(vocab_size, embedding_dim)\n                self.lstm = nn.LSTM(embedding_dim, 64, batch_first=True)\n                self.classifier = nn.Linear(64, 10)  # 10 action classes\n\n            def forward(self, x):\n                embedded = self.embedding(x)\n                lstm_out, (hidden, _) = self.lstm(embedded)\n                output = self.classifier(hidden[-1])\n                return torch.softmax(output, dim=1)\n\n        return SimpleTextModel()\n\n    def image_callback(self, msg):\n        \"\"\"Handle incoming image messages\"\"\"\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Store image with timestamp\n            timestamp = msg.header.stamp.sec + msg.header.stamp.nanosec * 1e-9\n            self.image_buffer.append({\n                'image': cv_image,\n                'timestamp': timestamp,\n                'msg': msg\n            })\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def text_callback(self, msg):\n        \"\"\"Handle incoming text messages\"\"\"\n        self.text_buffer.append({\n            'text': msg.data,\n            'timestamp': time.time()\n        })\n\n    def processing_callback(self):\n        \"\"\"Main processing callback\"\"\"\n        # Get latest synchronized data\n        sync_data = self.get_synchronized_data()\n\n        if sync_data:\n            # Add to processing queue\n            self.processing_queue.put(sync_data)\n\n    def get_synchronized_data(self) -> Optional[Dict]:\n        \"\"\"Get synchronized vision and language data\"\"\"\n        if not self.image_buffer or not self.text_buffer:\n            return None\n\n        # Find the most recent text and image within temporal window\n        text_data = self.text_buffer[-1]  # Most recent text\n        image_data = self.image_buffer[-1]  # Most recent image\n\n        # Check temporal alignment (within 1 second)\n        time_diff = abs(text_data['timestamp'] - image_data['timestamp'])\n\n        if time_diff <= 1.0:  # 1 second window\n            return {\n                'image_data': image_data,\n                'text_data': text_data,\n                'time_diff': time_diff\n            }\n\n        return None\n\n    def process_vision_language(self):\n        \"\"\"Process vision-language data in separate thread\"\"\"\n        while rclpy.ok():\n            try:\n                # Get next task from queue\n                try:\n                    sync_data = self.processing_queue.get(timeout=1.0)\n                    self.process_vision_language_pair(sync_data)\n                    self.processing_queue.task_done()\n                except queue.Empty:\n                    continue\n            except Exception as e:\n                self.get_logger().error(f'Error in vision-language processing: {e}')\n                time.sleep(0.1)\n\n    def process_vision_language_pair(self, sync_data: Dict):\n        \"\"\"Process a synchronized vision-language pair\"\"\"\n        image_data = sync_data['image_data']\n        text_data = sync_data['text_data']\n\n        try:\n            # Process image for object detection\n            detections = self.detect_objects(image_data['image'])\n\n            # Process text for understanding\n            text_analysis = self.analyze_text(text_data['text'])\n\n            # Combine vision and language information\n            combined_analysis = self.combine_vision_language(\n                detections, text_analysis, text_data['text']\n            )\n\n            # Publish results\n            self.publish_vision_language_results(\n                combined_analysis, image_data['msg']\n            )\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing vision-language pair: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Detect objects in the image\"\"\"\n        # Convert image for model input\n        input_tensor = self.preprocess_image(image)\n\n        with torch.no_grad():\n            class_scores, bbox_deltas = self.vision_model(input_tensor)\n\n        # Convert to bounding boxes and class predictions\n        height, width = image.shape[:2]\n\n        # For demonstration, create simulated detections\n        detections = []\n\n        # Simulate some object detections\n        for i in range(min(self.max_objects, 5)):  # Max 5 objects for demo\n            # Random bounding box\n            x = np.random.randint(0, width // 2)\n            y = np.random.randint(0, height // 2)\n            w = np.random.randint(width // 4, width // 2)\n            h = np.random.randint(height // 4, height // 2)\n\n            # Random class (simplified)\n            class_id = np.random.randint(0, 10)\n            confidence = np.random.uniform(0.6, 0.95)\n\n            detection = {\n                'bbox': [x, y, w, h],\n                'class_id': class_id,\n                'confidence': confidence,\n                'class_name': f'object_{class_id}'\n            }\n\n            if confidence > self.detection_threshold:\n                detections.append(detection)\n\n        return detections\n\n    def analyze_text(self, text):\n        \"\"\"Analyze text for semantic content\"\"\"\n        # In a real implementation, this would use NLP models\n        # For demonstration, we'll extract simple semantic information\n\n        analysis = {\n            'actions': [],\n            'objects': [],\n            'spatial_relations': [],\n            'quantities': []\n        }\n\n        # Simple keyword extraction\n        words = text.lower().split()\n\n        action_keywords = ['move', 'go', 'pick', 'place', 'turn', 'wave', 'grasp']\n        object_keywords = ['cup', 'table', 'chair', 'box', 'ball', 'book']\n        spatial_keywords = ['left', 'right', 'front', 'back', 'near', 'far', 'on', 'under', 'beside']\n\n        for word in words:\n            if word in action_keywords:\n                analysis['actions'].append(word)\n            elif word in object_keywords:\n                analysis['objects'].append(word)\n            elif word in spatial_keywords:\n                analysis['spatial_relations'].append(word)\n\n        return analysis\n\n    def combine_vision_language(self, detections, text_analysis, text_command):\n        \"\"\"Combine vision and language information\"\"\"\n        # Create scene graph based on combined information\n        scene_graph = {\n            'objects': [],\n            'relationships': [],\n            'actions': text_analysis['actions'],\n            'command': text_command\n        }\n\n        # Add detected objects to scene graph\n        for detection in detections:\n            obj_info = {\n                'id': len(scene_graph['objects']),\n                'name': detection['class_name'],\n                'bbox': detection['bbox'],\n                'confidence': detection['confidence'],\n                'center': [\n                    detection['bbox'][0] + detection['bbox'][2] // 2,\n                    detection['bbox'][1] + detection['bbox'][3] // 2\n                ]\n            }\n            scene_graph['objects'].append(obj_info)\n\n        # Determine spatial relationships based on text and vision\n        if text_analysis['spatial_relations'] and detections:\n            # Find relevant objects based on text\n            relevant_objects = []\n            for obj in scene_graph['objects']:\n                if any(keyword in obj['name'].lower() for keyword in text_analysis['objects']):\n                    relevant_objects.append(obj)\n\n            # Create spatial relationships\n            if len(relevant_objects) >= 2:\n                for i, obj1 in enumerate(relevant_objects):\n                    for j, obj2 in enumerate(relevant_objects[i+1:], i+1):\n                        # Calculate spatial relationship\n                        dx = obj2['center'][0] - obj1['center'][0]\n                        dy = obj2['center'][1] - obj1['center'][1]\n\n                        relation = {\n                            'subject': obj1['name'],\n                            'predicate': 'left_of' if dx < 0 else 'right_of',\n                            'object': obj2['name'],\n                            'distance': np.sqrt(dx*dx + dy*dy)\n                        }\n                        scene_graph['relationships'].append(relation)\n\n        return {\n            'scene_graph': scene_graph,\n            'detections': detections,\n            'text_analysis': text_analysis,\n            'spatial_relationships': scene_graph['relationships']\n        }\n\n    def publish_vision_language_results(self, analysis, original_msg):\n        \"\"\"Publish vision-language analysis results\"\"\"\n        # Publish object detections\n        detections_msg = String()\n        detections_msg.data = str(analysis['detections'])\n        self.object_detection_pub.publish(detections_msg)\n\n        # Publish spatial relationships\n        relationships_msg = String()\n        relationships_msg.data = str(analysis['spatial_relationships'])\n        self.spatial_relationships_pub.publish(relationships_msg)\n\n        # Publish scene graph\n        scene_graph_msg = String()\n        scene_graph_msg.data = str(analysis['scene_graph'])\n        self.scene_graph_pub.publish(scene_graph_msg)\n\n        # Process and publish annotated image\n        annotated_image = self.annotate_image(\n            self.image_buffer[-1]['image'] if self.image_buffer else np.zeros((480, 640, 3), dtype=np.uint8),\n            analysis['detections']\n        )\n\n        if annotated_image is not None:\n            annotated_msg = self.cv_bridge.cv2_to_imgmsg(annotated_image, encoding='bgr8')\n            annotated_msg.header = original_msg.header\n            self.processed_image_pub.publish(annotated_msg)\n\n    def preprocess_image(self, image):\n        \"\"\"Preprocess image for model input\"\"\"\n        # Resize and normalize image\n        resized = cv2.resize(image, (224, 224))\n        normalized = resized.astype(np.float32) / 255.0\n        transposed = np.transpose(normalized, (2, 0, 1))  # HWC to CHW\n        batched = np.expand_dims(transposed, axis=0)  # Add batch dimension\n\n        return torch.tensor(batched, dtype=torch.float32)\n\n    def annotate_image(self, image, detections):\n        \"\"\"Annotate image with detection results\"\"\"\n        annotated = image.copy()\n\n        for detection in detections:\n            x, y, w, h = detection['bbox']\n\n            # Draw bounding box\n            cv2.rectangle(annotated, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n            # Draw label\n            label = f\"{detection['class_name']}: {detection['confidence']:.2f}\"\n            cv2.putText(annotated, label, (x, y - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return annotated\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    processor = VisionLanguageProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info('Shutting down Vision-Language Processor')\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"action-selection-node",children:"Action Selection Node"}),"\n",(0,a.jsxs)(n.p,{children:["Create ",(0,a.jsx)(n.code,{children:"~/ros2_ws/src/multi_modal_integration/multi_modal_integration/action_selector.py"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, Point\nfrom sensor_msgs.msg import Image\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport time\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import deque\n\nclass ActionSelector(Node):\n    def __init__(self):\n        super().__init__('action_selector')\n\n        # Declare parameters\n        self.declare_parameter('action_frequency', 10.0)\n        self.declare_parameter('enable_learning', True)\n        self.declare_parameter('max_action_history', 50)\n        self.declare_parameter('confidence_threshold', 0.7)\n        self.declare_parameter('enable_safety_check', True)\n\n        # Get parameters\n        self.action_frequency = self.get_parameter('action_frequency').value\n        self.enable_learning = self.get_parameter('enable_learning').value\n        self.max_action_history = self.get_parameter('max_action_history').value\n        self.confidence_threshold = self.get_parameter('confidence_threshold').value\n        self.enable_safety_check = self.get_parameter('enable_safety_check').value\n\n        # Create subscribers\n        self.multi_modal_sub = self.create_subscription(\n            String,\n            '/multi_modal_output',\n            self.multi_modal_callback,\n            10\n        )\n\n        self.scene_graph_sub = self.create_subscription(\n            String,\n            '/scene_graph',\n            self.scene_graph_callback,\n            10\n        )\n\n        self.object_detection_sub = self.create_subscription(\n            String,\n            '/object_detections',\n            self.object_detection_callback,\n            10\n        )\n\n        # Create publishers\n        self.action_pub = self.create_publisher(\n            Twist,\n            '/selected_action',\n            10\n        )\n\n        self.action_description_pub = self.create_publisher(\n            String,\n            '/action_description',\n            10\n        )\n\n        self.action_log_pub = self.create_publisher(\n            String,\n            '/action_log',\n            10\n        )\n\n        # State buffers\n        self.multi_modal_data = None\n        self.scene_graph = None\n        self.object_detections = []\n        self.action_history = deque(maxlen=self.max_action_history)\n\n        # Initialize action selection model\n        self.action_model = self.initialize_action_model()\n\n        # Create timer for action selection\n        self.action_timer = self.create_timer(\n            1.0 / self.action_frequency,\n            self.action_selection_callback\n        )\n\n        self.get_logger().info('Action Selector initialized')\n\n    def initialize_action_model(self):\n        \"\"\"Initialize the action selection model\"\"\"\n        # In a real implementation, this would be a pre-trained model\n        # For demonstration, we'll create a simple model\n        class SimpleActionModel(nn.Module):\n            def __init__(self, input_dim=256, action_dim=6):\n                super().__init__()\n                self.network = nn.Sequential(\n                    nn.Linear(input_dim, 128),\n                    nn.ReLU(),\n                    nn.Linear(128, 64),\n                    nn.ReLU(),\n                    nn.Linear(64, action_dim)\n                )\n                self.action_names = ['move_forward', 'move_backward', 'turn_left',\n                                   'turn_right', 'grasp', 'release']\n\n            def forward(self, x):\n                action_logits = self.network(x)\n                action_probs = torch.softmax(action_logits, dim=-1)\n                return action_probs\n\n        return SimpleActionModel()\n\n    def multi_modal_callback(self, msg):\n        \"\"\"Handle multi-modal fusion results\"\"\"\n        try:\n            data = json.loads(msg.data)\n            self.multi_modal_data = data\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in multi-modal data: {msg.data}')\n\n    def scene_graph_callback(self, msg):\n        \"\"\"Handle scene graph updates\"\"\"\n        try:\n            self.scene_graph = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in scene graph: {msg.data}')\n\n    def object_detection_callback(self, msg):\n        \"\"\"Handle object detection updates\"\"\"\n        try:\n            self.object_detections = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(f'Invalid JSON in object detections: {msg.data}')\n\n    def action_selection_callback(self):\n        \"\"\"Main action selection callback\"\"\"\n        if not self.multi_modal_data:\n            return\n\n        try:\n            # Select action based on multi-modal input\n            action, confidence, description = self.select_action()\n\n            if action is not None and confidence > self.confidence_threshold:\n                # Apply safety checks\n                if not self.enable_safety_check or self.is_action_safe(action):\n                    # Execute action\n                    self.execute_action(action, description)\n\n                    # Log the action\n                    self.log_action(action, description, confidence)\n                else:\n                    self.get_logger().warn('Action failed safety check')\n            else:\n                self.get_logger().debug(f'Action confidence too low: {confidence:.2f}')\n\n        except Exception as e:\n            self.get_logger().error(f'Error in action selection: {e}')\n\n    def select_action(self) -> Tuple[Optional[Twist], float, str]:\n        \"\"\"Select the best action based on multi-modal input\"\"\"\n        # Extract features from multi-modal data\n        fused_features = self.multi_modal_data.get('fused_features', [])\n\n        if not fused_features:\n            return None, 0.0, \"No fused features available\"\n\n        # Convert to tensor for model\n        features_tensor = torch.tensor(fused_features, dtype=torch.float32).unsqueeze(0)\n\n        with torch.no_grad():\n            action_probs = self.action_model(features_tensor)\n\n        # Get the most probable action\n        action_idx = torch.argmax(action_probs, dim=-1).item()\n        confidence = action_probs[0][action_idx].item()\n        action_name = self.action_model.action_names[action_idx]\n\n        # Convert action name to Twist command\n        action_cmd = self.action_name_to_twist(action_name)\n\n        # Generate description\n        description = self.generate_action_description(\n            action_name, confidence, self.multi_modal_data.get('input_text', '')\n        )\n\n        return action_cmd, confidence, description\n\n    def action_name_to_twist(self, action_name: str) -> Twist:\n        \"\"\"Convert action name to Twist command\"\"\"\n        cmd = Twist()\n\n        if action_name == 'move_forward':\n            cmd.linear.x = 0.2  # m/s\n        elif action_name == 'move_backward':\n            cmd.linear.x = -0.2\n        elif action_name == 'turn_left':\n            cmd.angular.z = 0.5  # rad/s\n        elif action_name == 'turn_right':\n            cmd.angular.z = -0.5\n        elif action_name == 'grasp':\n            # In a real system, this would be a different command type\n            cmd.linear.z = 0.1  # Indicate grasp action\n        elif action_name == 'release':\n            cmd.linear.z = -0.1  # Indicate release action\n\n        return cmd\n\n    def generate_action_description(self, action_name: str, confidence: float, command: str) -> str:\n        \"\"\"Generate human-readable action description\"\"\"\n        return f\"Selected action '{action_name}' with {confidence:.2f} confidence based on command: '{command}'\"\n\n    def is_action_safe(self, action: Twist) -> bool:\n        \"\"\"Perform safety checks on the action\"\"\"\n        # Check for potentially dangerous actions\n        if abs(action.linear.x) > 0.5:  # Too fast\n            return False\n        if abs(action.angular.z) > 1.0:  # Too fast rotation\n            return False\n        if action.linear.z != 0 and abs(action.linear.z) > 0.2:  # Extreme vertical movement\n            return False\n\n        # In a real implementation, this would check for obstacles, etc.\n        return True\n\n    def execute_action(self, action: Twist, description: str):\n        \"\"\"Execute the selected action\"\"\"\n        self.action_pub.publish(action)\n\n        # Publish description\n        desc_msg = String()\n        desc_msg.data = description\n        self.action_description_pub.publish(desc_msg)\n\n        self.get_logger().info(f'Executing action: {description}')\n\n    def log_action(self, action: Twist, description: str, confidence: float):\n        \"\"\"Log the action for learning and debugging\"\"\"\n        log_entry = {\n            'timestamp': time.time(),\n            'action': {\n                'linear': {\n                    'x': action.linear.x,\n                    'y': action.linear.y,\n                    'z': action.linear.z\n                },\n                'angular': {\n                    'x': action.angular.x,\n                    'y': action.angular.y,\n                    'z': action.angular.z\n                }\n            },\n            'description': description,\n            'confidence': confidence,\n            'multi_modal_input': self.multi_modal_data\n        }\n\n        # Add to history\n        self.action_history.append(log_entry)\n\n        # Publish log\n        log_msg = String()\n        log_msg.data = json.dumps(log_entry)\n        self.action_log_pub.publish(log_msg)\n\n        # In a learning-enabled system, this would update the model\n        if self.enable_learning:\n            self.update_action_model(log_entry)\n\n    def update_action_model(self, log_entry: Dict):\n        \"\"\"Update the action model based on experience\"\"\"\n        # In a real implementation, this would perform online learning\n        # For demonstration, we'll just log that learning would occur\n        self.get_logger().debug('Action model update would occur here')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    selector = ActionSelector()\n\n    try:\n        rclpy.spin(selector)\n    except KeyboardInterrupt:\n        selector.get_logger().info('Shutting down Action Selector')\n    finally:\n        selector.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"active-learning-exercise",children:"Active Learning Exercise"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Exercise: Cross-Modal Attention Visualization"})}),"\n",(0,a.jsx)(n.p,{children:"Implement and experiment with different cross-modal attention mechanisms:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Visual Grounding"}),": Create a system that highlights relevant image regions based on text commands"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Text Grounding"}),": Implement text highlighting based on visual attention"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Temporal Attention"}),": Design attention mechanisms that consider temporal context"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Multi-Head Attention"}),": Implement multi-head attention for different aspects of multimodal processing"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Visualize the attention weights and evaluate how well your system grounds language in visual perception."}),"\n",(0,a.jsx)(n.h2,{id:"worked-example-black-box-to-glass-box---multi-modal-integration-system",children:"Worked Example: Black-box to Glass-box - Multi-Modal Integration System"}),"\n",(0,a.jsx)(n.h3,{id:"black-box-view",children:"Black-box View"}),"\n",(0,a.jsx)(n.p,{children:"We'll create a complete multi-modal integration system that takes visual input and natural language commands, then produces appropriate robot actions. The black-box view is: the system receives images and text, and outputs robot commands."}),"\n",(0,a.jsx)(n.h3,{id:"glass-box-implementation",children:"Glass-box Implementation"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Complete system architecture:"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The implementation includes:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Multi-modal fusion combining vision and language"}),"\n",(0,a.jsx)(n.li,{children:"Cross-modal attention mechanisms"}),"\n",(0,a.jsx)(n.li,{children:"Action selection based on fused information"}),"\n",(0,a.jsx)(n.li,{children:"Safety and validation systems"}),"\n",(0,a.jsx)(n.li,{children:"Temporal integration for coherent behavior"}),"\n"]}),"\n",(0,a.jsxs)(n.ol,{start:"2",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Advanced fusion mechanisms:"})}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# Example of advanced multi-modal fusion with attention\nclass AdvancedMultiModalFusion:\n    def __init__(self, visual_dim=512, text_dim=512, fusion_dim=512):\n        # Multi-head attention for cross-modal interaction\n        self.num_heads = 8\n        self.head_dim = fusion_dim // self.num_heads\n\n        # Vision and text encoders\n        self.visual_encoder = nn.Linear(visual_dim, fusion_dim)\n        self.text_encoder = nn.Linear(text_dim, fusion_dim)\n\n        # Multi-head attention layers\n        self.vision_attention = nn.MultiheadAttention(\n            embed_dim=fusion_dim,\n            num_heads=self.num_heads,\n            batch_first=True\n        )\n        self.text_attention = nn.MultiheadAttention(\n            embed_dim=fusion_dim,\n            num_heads=self.num_heads,\n            batch_first=True\n        )\n\n        # Fusion network\n        self.fusion_network = nn.Sequential(\n            nn.Linear(fusion_dim * 2, fusion_dim),\n            nn.ReLU(),\n            nn.Linear(fusion_dim, fusion_dim // 2),\n            nn.ReLU(),\n            nn.Linear(fusion_dim // 2, fusion_dim // 4)\n        )\n\n    def forward(self, visual_features, text_features):\n        # Encode features\n        vis_encoded = torch.relu(self.visual_encoder(visual_features))\n        text_encoded = torch.relu(self.text_encoder(text_features))\n\n        # Apply cross-attention\n        vis_attn_out, vis_attn_weights = self.vision_attention(\n            query=vis_encoded,\n            key=text_encoded,\n            value=text_encoded\n        )\n\n        text_attn_out, text_attn_weights = self.text_attention(\n            query=text_encoded,\n            key=vis_encoded,\n            value=vis_encoded\n        )\n\n        # Concatenate attended features\n        fused_features = torch.cat([vis_attn_out, text_attn_out], dim=-1)\n\n        # Apply fusion network\n        output = self.fusion_network(fused_features)\n\n        return output, (vis_attn_weights, text_attn_weights)\n"})}),"\n",(0,a.jsxs)(n.ol,{start:"3",children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Temporal integration:"})}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The system maintains temporal context across modalities:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Memory buffers for recent observations"}),"\n",(0,a.jsx)(n.li,{children:"Temporal attention for sequence understanding"}),"\n",(0,a.jsx)(n.li,{children:"State tracking for coherent behavior"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"understanding-the-implementation",children:"Understanding the Implementation"}),"\n",(0,a.jsx)(n.p,{children:"The glass-box view reveals:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"The system fuses information across multiple modalities using attention mechanisms"}),"\n",(0,a.jsx)(n.li,{children:"Cross-modal grounding enables language to guide visual attention"}),"\n",(0,a.jsx)(n.li,{children:"Temporal integration maintains coherent behavior over time"}),"\n",(0,a.jsx)(n.li,{children:"Safety mechanisms ensure safe action execution"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"tiered-assessments",children:"Tiered Assessments"}),"\n",(0,a.jsx)(n.h3,{id:"tier-1-basic-understanding",children:"Tier 1: Basic Understanding"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What is multi-modal integration and why is it important for humanoid robots?"}),"\n",(0,a.jsx)(n.li,{children:"Name three modalities commonly integrated in robotic systems."}),"\n",(0,a.jsx)(n.li,{children:"What is cross-modal attention and how does it work?"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tier-2-application",children:"Tier 2: Application"}),"\n",(0,a.jsxs)(n.ol,{start:"4",children:["\n",(0,a.jsx)(n.li,{children:"Implement a multi-modal fusion system that combines visual and linguistic information."}),"\n",(0,a.jsx)(n.li,{children:"Create a cross-modal attention mechanism that grounds language in visual perception."}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"tier-3-analysis-and-synthesis",children:"Tier 3: Analysis and Synthesis"}),"\n",(0,a.jsxs)(n.ol,{start:"6",children:["\n",(0,a.jsx)(n.li,{children:"Design a complete multi-modal system for a humanoid robot that integrates vision, language, and action with temporal coherence and safety validation."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"mermaid-diagram",children:"Mermaid Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[Visual Input] --\x3e B[Visual Processing]\n    C[Linguistic Input] --\x3e D[Linguistic Processing]\n    B --\x3e E[Multi-Modal Fusion]\n    D --\x3e E\n    E --\x3e F[Cross-Modal Attention]\n    F --\x3e G[Action Selection]\n    G --\x3e H[Robot Actions]\n    H --\x3e I[Feedback Loop]\n    I --\x3e A\n    I --\x3e C\n\n    B --\x3e J[Spatial Understanding]\n    D --\x3e K[Semantic Understanding]\n    J --\x3e E\n    K --\x3e E\n\n    F --\x3e L[Temporal Integration]\n    L --\x3e G\n\n    style A fill:#ff9999\n    style C fill:#ff9999\n    style H fill:#99ccff\n    style E fill:#99ff99\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Alt-text for diagram:"}),' "Multi-modal integration system showing visual input and linguistic input flowing to visual processing and linguistic processing respectively. Both outputs flow to multi-modal fusion, which connects to cross-modal attention, action selection, and robot actions. Robot actions connect to feedback loop which flows back to both visual and linguistic inputs. Visual processing also connects to spatial understanding and linguistic processing connects to semantic understanding, both feeding into multi-modal fusion. Cross-modal attention connects to temporal integration which feeds to action selection. Visual and linguistic inputs are highlighted in pink, robot actions in light blue, and multi-modal fusion in light green."']}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covered the implementation of multi-modal integration systems that combine vision, language, and action for humanoid robots. We explored the theoretical foundations of cross-modal attention and fusion, implemented practical systems for processing and integrating multiple sensory modalities, and demonstrated how to create coherent behavior through temporal integration. The examples showed how to build systems that can understand complex scenes described in natural language and execute appropriate actions based on multi-modal perception."}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., ... & Sutskever, I. (2021). Learning transferable visual models from natural language supervision. ",(0,a.jsx)(n.em,{children:"International Conference on Machine Learning"}),", 8748-8763."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Chen, C. Y., Rohrbach, M., Shin, S., Yan, T., Shrivastava, A., & Darrell, T. (2019). Neuro-symbolic language grounding for video description. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:1906.07325"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Lu, J., Goswami, V., & Lee, S. (2019). 12-in-1: Multi-task vision and language representation learning. ",(0,a.jsx)(n.em,{children:"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"}),", 10439-10448."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Li, B., Wu, F., Swersky, K., Zhang, L., & Goldberg, K. (2022). Cogview: Mastering text-to-image generation via transformers. ",(0,a.jsx)(n.em,{children:"Advances in Neural Information Processing Systems"}),", 35, 19822-19835."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Alayrac, J. B., Donahue, J., Laptev, I., Miech, A., Prince, C., & Sivic, J. (2022). Flamingo: A visual language model for few-shot learning. ",(0,a.jsx)(n.em,{children:"arXiv preprint arXiv:2204.14198"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Zeng, A., Florence, P., Lee, R., Tompson, J., Makadia, A., Soh, S., ... & Ichnowski, J. (2022). Robotic skill learning from videos using contrastive learning. ",(0,a.jsx)(n.em,{children:"Conference on Robot Learning"}),", 1588-1601."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Huang, W., Abbeel, P., Pathak, D., & Mordatch, I. (2022). Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ",(0,a.jsx)(n.em,{children:"International Conference on Machine Learning"}),", 9118-9147."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Misra, D., Lang, J., Grizou, J., Shridhar, M., & Fox, D. (2022). Mapping natural language instructions to mobile manipulation actions. ",(0,a.jsx)(n.em,{children:"IEEE Robotics and Automation Letters"}),", 7(2), 3960-3967."]}),"\n"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var i=t(6540);const a={},s=i.createContext(a);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);